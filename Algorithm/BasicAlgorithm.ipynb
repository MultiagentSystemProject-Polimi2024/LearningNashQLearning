{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nash Q learning basic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nashpy as nash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 4 #Number of games\n",
    "N = 2 #Number of players\n",
    "A = 2 #Number of actions per player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player1 action / Player2 action / starting state / ending state / probability\n",
    "#Player 1 - column player - 0: .3, 1: .4\n",
    "#Player 2 - row player - 0: .1, 1: .2\n",
    "TRANSITION_MATRIX = np.array(\n",
    "    [\n",
    "        # Player 1 - Action 0\n",
    "        [\n",
    "            # Player 2 - Action 0 - .1 .3\n",
    "            [\n",
    "                \n",
    "                [0, 0.5, 0, 0.5],\n",
    "                [0, 1, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ],\n",
    "\n",
    "            # Player 2 - Action 1 .2 .3\n",
    "            [\n",
    "                \n",
    "                [1, 0, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ]\n",
    "        ],\n",
    "\n",
    "        # Player 1 - Action 1\n",
    "        [\n",
    "            # Player 2 - Action 0 .1 .4\n",
    "            [\n",
    "                \n",
    "                [1, 0, 0, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [0, 0, 0, 1],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ],\n",
    "\n",
    "            # Player 2 - Action 1 .2 .4\n",
    "            [\n",
    "                \n",
    "                [0, 0.5, 0, 0.5],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSITION_MATRIX[0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state / player1 action / player2 action / [player1 reward, player2 reward]\n",
    "PAYOFF_MATRIX = np.array([\n",
    "    # State 0\n",
    "    [\n",
    "        [ [2, 1], [0, 0] ],\n",
    "        [ [0, 0], [1, 2] ]\n",
    "    ],\n",
    "    # State 1\n",
    "    [\n",
    "        [ [1, 1], [3, 0] ],\n",
    "        [ [0, 3], [2, 2] ]\n",
    "    ],\n",
    "    # State \n",
    "    [\n",
    "        [ [2, 0], [0, 2] ],\n",
    "        [ [0, 1], [1, 0] ]\n",
    "    ],\n",
    "    # State 33\n",
    "    [\n",
    "        [ [1, 1], [0, 0] ],\n",
    "        [ [0, 0], [2, 2] ]\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAYOFF_MATRIX[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_probability(state, player1_action, player2_action, next):\n",
    "    return TRANSITION_MATRIX[player1_action, player2_action, state, next]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state, player1_action, player2_action):\n",
    "    return PAYOFF_MATRIX[state, player1_action, player2_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 10000\n",
    "state = 0\n",
    "totalReward = np.array([0, 0])\n",
    "randomRewards = []\n",
    "for _ in range(n_games):\n",
    "    print(\"State:\", state)\n",
    "    player1_action = np.random.choice(A, p=[0.5, 0.5])\n",
    "    player2_action = np.random.choice(A, p=[0.5, 0.5])\n",
    "    print(\"Player 1 action:\", player1_action)\n",
    "    print(\"Player 2 action:\", player2_action)\n",
    "    next_state = np.random.choice(range(Q), p=TRANSITION_MATRIX[player1_action, player2_action, state])\n",
    "    print(\"Next state:\", next_state)\n",
    "    r = reward(state, player1_action, player2_action)\n",
    "    print(\"Reward:\", r)\n",
    "    state = next_state\n",
    "    totalReward += r\n",
    "    randomRewards.append(r)\n",
    "print(\"Total reward:\", totalReward / n_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNashEq(state, payoff_matrix):\n",
    "    game = nash.Game(payoff_matrix[state, :, :, 0], PAYOFF_MATRIX[state, :, :, 1])\n",
    "    eqs = game.vertex_enumeration()\n",
    "\n",
    "    try:\n",
    "        eq = next(eqs)\n",
    "        return np.abs(eq)\n",
    "    except Exception:\n",
    "        a = 1\n",
    "        b = 0\n",
    "        return [[a, 1 - a], [b, 1 - b]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeNashEq(3, PAYOFF_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate plays with nash policy\n",
    "state = 0\n",
    "totalReward = np.array([0, 0])\n",
    "localNashRewards = []\n",
    "EPSILON = 0.1\n",
    "\n",
    "for _ in range(n_games):\n",
    "    print(\"State:\", state)\n",
    "    nashEq = computeNashEq(state, PAYOFF_MATRIX)\n",
    "    print(\"Nash equilibrium:\", nashEq)\n",
    "    player1_action = np.random.choice(A, p=nashEq[0]) if np.random.rand() > EPSILON else np.random.choice(A)\n",
    "    player2_action = np.random.choice(A, p=nashEq[1]) if np.random.rand() > EPSILON else np.random.choice(A)\n",
    "    print(\"Player 1 action:\", player1_action)\n",
    "    print(\"Player 2 action:\", player2_action)\n",
    "    next_state = np.random.choice(range(Q), p=TRANSITION_MATRIX[player1_action, player2_action, state])\n",
    "    print(\"Next state:\", next_state)\n",
    "    r = reward(state, player1_action, player2_action)\n",
    "    print(\"Reward:\", r)\n",
    "    state = next_state\n",
    "    totalReward += r\n",
    "    localNashRewards.append(r)\n",
    "print(\"Total reward:\", totalReward/n_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qTables = [ np.zeros((Q, A, A, N)) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    computeNashEq(0, qTables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectedPayoff(payoff_matrix, player1_strategy, player2_strategy):\n",
    "    expected_payoff = np.dot(np.dot(player1_strategy, payoff_matrix), player2_strategy)\n",
    "    return expected_payoff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PAYOFF_MATRIX[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedPayoff(PAYOFF_MATRIX[2, :, :, 1], np.array([0.5, .5]), np.array([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NashQ\n",
    "ALPHA = 0.5\n",
    "GAMMA = 0.8\n",
    "EPSILON = 0.1\n",
    "PURE_TRAINING_EPISODES = 1000\n",
    "# n_games = 5000\n",
    "\n",
    "qTables = [np.zeros((Q, A, A, N)) for _ in range(N)] #QTable for each player\n",
    "\n",
    "state = 0\n",
    "totalReward = [np.array([0, 0]) for _ in range(N)]\n",
    "diffs = [[]for _ in range(N)]\n",
    "NashQRewards = [[]for _ in range(N)]\n",
    "NashEquilibria = [[]for _ in range(N)]\n",
    "\n",
    "for t in range(n_games):\n",
    "   ALPHA = 1 / (t + 1 - PURE_TRAINING_EPISODES) if t >= PURE_TRAINING_EPISODES else ALPHA\n",
    "   player1_action = np.random.choice(A, p=nashEq[0]) if np.random.rand() > EPSILON else np.random.choice(A)\n",
    "   player2_action = np.random.choice(A, p=nashEq[1]) if np.random.rand() > EPSILON else np.random.choice(A)\n",
    "   print(\"Player 1 action:\", player1_action)\n",
    "   print(\"Player 2 action:\", player2_action)\n",
    "   next_state = np.random.choice(range(Q), p=TRANSITION_MATRIX[player1_action, player2_action, state])\n",
    "   print(\"Next state:\", next_state)\n",
    "   r = reward(state, player1_action, player2_action)\n",
    "   print(\"Reward:\", r)\n",
    "   for i in range(N):\n",
    "      qTable = qTables[i]\n",
    "\n",
    "      print(\"State:\", state)\n",
    "      nashEq = np.abs(computeNashEq(state, qTable))\n",
    "      # print(\"Nash equilibrium:\", nashEq)\n",
    "      NashEquilibria[i].append(nashEq)\n",
    "\n",
    "      next_NashEq = computeNashEq(next_state, qTable)\n",
    "      next_qVal_0 = expectedPayoff(qTable[next_state, :, :, 0], next_NashEq[0], next_NashEq[1])\n",
    "      next_qVal_1 = expectedPayoff(qTable[next_state, :, :, 1], next_NashEq[0], next_NashEq[1])\n",
    "      oldQ = qTable[state, player1_action, player2_action].copy()\n",
    "      qTable[state, player1_action, player2_action, 0] = (1 - ALPHA) * qTable[state, player1_action, player2_action, 0] + ALPHA * (r[0] + GAMMA * next_qVal_0)\n",
    "      qTable[state, player1_action, player2_action, 1] = (1 - ALPHA) * qTable[state, player1_action, player2_action, 1] + ALPHA * (r[1] + GAMMA * next_qVal_1)\n",
    "\n",
    "      diffs[i].append(qTable[state, player1_action, player2_action] - oldQ)\n",
    "      # print(\"QTable:\", qTable[state])\n",
    "      \n",
    "      totalReward[i] += r\n",
    "      NashQRewards[i].append(r)\n",
    "   state = next_state\n",
    "for i in range(N):\n",
    "   print(\"Expected reward:\", totalReward[i]/n_games)\n",
    "for i in range(N):\n",
    "   print(\"QTable:\", qTables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    diffs[i] = np.array(diffs[i])\n",
    "    print(diffs[i].shape)\n",
    "    diffs[i]\n",
    "    NashQRewards[i] = np.array(NashQRewards[i])\n",
    "\n",
    "randomRewards = np.array(randomRewards)\n",
    "localNashRewards = np.array(localNashRewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    sns.set_theme()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"QTable diffs over time player 1 qtable \"+ str(i))\n",
    "    sns.lineplot(data=diffs[i][:, 0])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"QTable diffs over time player 2 qtable \"+ str(i))\n",
    "    sns.lineplot(data=diffs[i][:, 1])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Rewards over time per player 0\")\n",
    "    sns.lineplot(data=randomRewards[:, 0], label=\"Random\")\n",
    "    sns.lineplot(data=localNashRewards[:, 0], label=\"Local Nash\")\n",
    "    sns.lineplot(data=NashQRewards[i][:, 0], label=\"Nash Q \"+str(i))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Rewards over time per player 1\")\n",
    "    sns.lineplot(data=randomRewards[:, 1], label=\"Random\")\n",
    "    sns.lineplot(data=localNashRewards[:, 1], label=\"Local Nash\")\n",
    "    sns.lineplot(data=NashQRewards[i][:, 1], label=\"Nash Q \"+str(i))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth the data\n",
    "for i in range(N):\n",
    "    window = 200\n",
    "    cumsum = np.cumsum(randomRewards[:, 0])\n",
    "    cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "    randomRewardsSmooth_0 = cumsum[window - 1:] / window\n",
    "\n",
    "    cumsum = np.cumsum(localNashRewards[:, 0])\n",
    "    cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "    localNashRewardsSmooth_0 = cumsum[window - 1:] / window\n",
    "\n",
    "    cumsum = np.cumsum(NashQRewards[i][:, 0])\n",
    "    cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "    NashQRewardsSmooth_0 = cumsum[window - 1:] / window\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Smoothed Rewards over time per player 0 - windowSize {window}\")\n",
    "    sns.lineplot(data=randomRewardsSmooth_0, label=\"Random\")\n",
    "    sns.lineplot(data=localNashRewardsSmooth_0, label=\"Local Nash\")\n",
    "    sns.lineplot(data=NashQRewardsSmooth_0, label=\"Nash Q \"+str(i))\n",
    "    plt.ylim(0, 2.5)\n",
    "    plt.xlabel(\"Number of played games\")\n",
    "    plt.ylabel(\"Reward for player 0\")\n",
    "    plt.show()\n",
    "\n",
    "    cumsum = np.cumsum(randomRewards[:, 1])\n",
    "    cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "    randomRewardsSmooth_1 = cumsum[window - 1:] / window\n",
    "\n",
    "    cumsum = np.cumsum(localNashRewards[:, 1])\n",
    "    cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "    localNashRewardsSmooth_1 = cumsum[window - 1:] / window\n",
    "\n",
    "    cumsum = np.cumsum(NashQRewards[i][:, 1])\n",
    "    cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "    NashQRewardsSmooth_1 = cumsum[window - 1:] / window\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Smoothed Rewards over time per player 1 - windowSize {window}\")\n",
    "    sns.lineplot(data=randomRewardsSmooth_1, label=\"Random\")\n",
    "    sns.lineplot(data=localNashRewardsSmooth_1, label=\"Local Nash\")\n",
    "    sns.lineplot(data=NashQRewardsSmooth_1, label=\"Nash Q \"+str(i))\n",
    "    plt.ylim(0, 2.5)\n",
    "    plt.xlabel(\"Number of played games\")\n",
    "    plt.ylabel(\"Reward for player 0\")\n",
    "    plt.show()\n",
    "\n",
    "    randomRewardsSmooth_sum = randomRewardsSmooth_0 + randomRewardsSmooth_1\n",
    "    localNashRewardsSmooth_sum = localNashRewardsSmooth_0 + localNashRewardsSmooth_1\n",
    "    NashQRewardsSmooth_sum = NashQRewardsSmooth_0 + NashQRewardsSmooth_1\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Smoothed sum of Rewards over time - windowSize {window}\")\n",
    "    sns.lineplot(data=randomRewardsSmooth_sum, label=\"Random\")\n",
    "    sns.lineplot(data=localNashRewardsSmooth_sum, label=\"Local Nash\")\n",
    "    sns.lineplot(data=NashQRewardsSmooth_sum, label=\"Nash Q\")\n",
    "    plt.ylim(0, 4.5)\n",
    "    plt.xlabel(\"Number of played games\")\n",
    "    plt.ylabel(\"Reward for player 0\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why are the two qTables different??\n",
    "In my opinion this is related to the computation of Nash Equilibria, which probably are NOT returned always in the same order (otherwise why)???\n",
    "\n",
    "08/04\n",
    "EUREKA!!!!\n",
    "there was a bug: the state was updated inside the second loop, the instruction of change of state was moved from line 43 to line 46 of cell of nashQ learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here I show that there are no differences between the rewards obtained\n",
    "for x in (NashQRewards[0]-NashQRewards[1]):\n",
    "    print(x)\n",
    "    for n in x:\n",
    "        assert n == 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I show that the Qtables contain the same values\n",
    "for x in (qTables[0]-qTables[1]):\n",
    "    for n in x:\n",
    "        for m in n:\n",
    "            for k in m:\n",
    "                print(k)\n",
    "                assert k == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this segment proves that the nash equilibria found in the same conditions are always the same\n",
    "for i in range(N):\n",
    "    NashEquilibria[i] = np.array(NashEquilibria[i])\n",
    "for x in NashEquilibria[0]-NashEquilibria[1]:\n",
    "    for a in x:\n",
    "        for b in a:\n",
    "            print(b)\n",
    "            assert b == 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
