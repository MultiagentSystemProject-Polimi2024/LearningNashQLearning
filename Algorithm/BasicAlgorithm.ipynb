{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nash Q learning basic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nashpy as nash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 4#Number of games\n",
    "N = 2 #Number of players\n",
    "A = 2 #Number of actions per player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Player1 action / Player2 action / starting state / ending state / probability\n",
    "#Player 1 - column player - 0: .3, 1: .4\n",
    "#Player 2 - row player - 0: .1, 1: .2\n",
    "TRANSITION_MATRIX = np.array(\n",
    "    [\n",
    "        # Player 1 - Action 0\n",
    "        [\n",
    "            # Player 2 - Action 0 - .1 .3\n",
    "            [\n",
    "                \n",
    "                [0, 0.5, 0, 0.5],\n",
    "                [0, 1, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ],\n",
    "\n",
    "            # Player 2 - Action 1 .2 .3\n",
    "            [\n",
    "                \n",
    "                [1, 0, 0, 0],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ]\n",
    "        ],\n",
    "\n",
    "        # Player 1 - Action 1\n",
    "        [\n",
    "            # Player 2 - Action 0 .1 .4\n",
    "            [\n",
    "                \n",
    "                [1, 0, 0, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [0, 0, 0, 1],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ],\n",
    "\n",
    "            # Player 2 - Action 1 .2 .4\n",
    "            [\n",
    "                \n",
    "                [0, 0.5, 0, 0.5],\n",
    "                [0, 0, 1, 0],\n",
    "                [0, 0, 0, 1],\n",
    "                [0.5, 0, 0, 0.5]\n",
    "                \n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSITION_MATRIX[0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state / player1 action / player2 action / [player1 reward, player2 reward]\n",
    "PAYOFF_MATRIX = np.array([\n",
    "    # State 0\n",
    "    [\n",
    "        [ [2, 1], [0, 0] ],\n",
    "        [ [0, 0], [1, 2] ]\n",
    "    ],\n",
    "    # State 1\n",
    "    [\n",
    "        [ [1, 1], [3, 0] ],\n",
    "        [ [0, 3], [2, 2] ]\n",
    "    ],\n",
    "    # State \n",
    "    [\n",
    "        [ [2, 0], [0, 2] ],\n",
    "        [ [0, 1], [1, 0] ]\n",
    "    ],\n",
    "    # State 33\n",
    "    [\n",
    "        [ [1, 1], [0, 0] ],\n",
    "        [ [0, 0], [2, 2] ]\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAYOFF_MATRIX[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_probability(state, player1_action, player2_action, next):\n",
    "    return TRANSITION_MATRIX[player1_action, player2_action, state, next]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state, player1_action, player2_action):\n",
    "    return PAYOFF_MATRIX[state, player1_action, player2_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "State: 2\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "State: 3\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "State: 1\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "State: 2\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "State: 3\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "State: 0\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [2 1]\n",
      "Total reward: [0.8636 0.838 ]\n"
     ]
    }
   ],
   "source": [
    "n_games = 5000\n",
    "state = 0\n",
    "totalReward = np.array([0, 0])\n",
    "randomRewards = []\n",
    "for _ in range(n_games):\n",
    "    print(\"State:\", state)\n",
    "    player1_action = np.random.choice(A, p=[0.5, 0.5])\n",
    "    player2_action = np.random.choice(A, p=[0.5, 0.5])\n",
    "    print(\"Player 1 action:\", player1_action)\n",
    "    print(\"Player 2 action:\", player2_action)\n",
    "    next_state = np.random.choice(range(Q), p=TRANSITION_MATRIX[player1_action, player2_action, state])\n",
    "    print(\"Next state:\", next_state)\n",
    "    r = reward(state, player1_action, player2_action)\n",
    "    print(\"Reward:\", r)\n",
    "    state = next_state\n",
    "    totalReward += r\n",
    "    randomRewards.append(r)\n",
    "print(\"Total reward:\", totalReward / n_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNashEq(state, payoff_matrix):\n",
    "    game = nash.Game(payoff_matrix[state, :, :, 0], PAYOFF_MATRIX[state, :, :, 1])\n",
    "    eqs = game.vertex_enumeration()\n",
    "\n",
    "    try:\n",
    "        eq = next(eqs)\n",
    "        return eq\n",
    "    except StopIteration:\n",
    "        a = np.random.rand()\n",
    "        b = np.random.rand()\n",
    "        return [[a, 1 - a], [b, 1 - b]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([0., 1.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeNashEq(3, PAYOFF_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n",
      "Nash equilibrium: (array([0., 1.]), array([0., 1.]))\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "State: 3\n",
      "Nash equilibrium: (array([0., 1.]), array([0., 1.]))\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "State: 0\n",
      "Nash equilibrium: (array([0., 1.]), array([0., 1.]))\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "State: 1\n",
      "Nash equilibrium: (array([1., 0.]), array([1., 0.]))\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "Total reward: [1.0002 1.0006]\n"
     ]
    }
   ],
   "source": [
    "#Simulate plays with nash policy\n",
    "state = 0\n",
    "totalReward = np.array([0, 0])\n",
    "localNashRewards = []\n",
    "for _ in range(n_games):\n",
    "    print(\"State:\", state)\n",
    "    nashEq = computeNashEq(state, PAYOFF_MATRIX)\n",
    "    print(\"Nash equilibrium:\", nashEq)\n",
    "    player1_action = np.random.choice(A, p=nashEq[0])\n",
    "    player2_action = np.random.choice(A, p=nashEq[1])\n",
    "    print(\"Player 1 action:\", player1_action)\n",
    "    print(\"Player 2 action:\", player2_action)\n",
    "    next_state = np.random.choice(range(Q), p=TRANSITION_MATRIX[player1_action, player2_action, state])\n",
    "    print(\"Next state:\", next_state)\n",
    "    r = reward(state, player1_action, player2_action)\n",
    "    print(\"Reward:\", r)\n",
    "    state = next_state\n",
    "    totalReward += r\n",
    "    localNashRewards.append(r)\n",
    "print(\"Total reward:\", totalReward/n_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qTable = np.zeros((Q, A, A, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nashpy\\polytope\\polytope.py:109: RuntimeWarning: divide by zero encountered in divide\n",
      "  hs = HalfspaceIntersection(halfspaces, feasible_point)\n",
      "C:\\Users\\Paolo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nashpy\\polytope\\polytope.py:109: RuntimeWarning: invalid value encountered in divide\n",
      "  hs = HalfspaceIntersection(halfspaces, feasible_point)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.31583955672772657, 0.6841604432722734],\n",
       " [0.23682927849405322, 0.7631707215059468]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeNashEq(0, qTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectedPayoff(payoff_matrix, player1_strategy, player2_strategy):\n",
    "    expected_payoff = np.dot(np.dot(player1_strategy, payoff_matrix), player2_strategy)\n",
    "    return expected_payoff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2 1]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 2]]]\n"
     ]
    }
   ],
   "source": [
    "print(PAYOFF_MATRIX[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectedPayoff(PAYOFF_MATRIX[2, :, :, 1], np.array([0.5, .5]), np.array([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n",
      "Nash equilibrium: [[0.12589589 0.87410411]\n",
      " [0.71918055 0.28081945]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.21150016 0.78849984]\n",
      " [0.95415369 0.04584631]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.71696998 0.28303002]\n",
      " [0.33636981 0.66363019]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.  0. ]\n",
      "  [0.  0. ]]\n",
      "\n",
      " [[0.  0. ]\n",
      "  [0.5 1. ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.13603868 0.86396132]\n",
      " [0.93187101 0.06812899]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[0.  0. ]\n",
      "  [0.  0. ]]\n",
      "\n",
      " [[0.  1.5]\n",
      "  [0.  0. ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.06276829 0.93723171]\n",
      " [0.73653073 0.26346927]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.  0. ]\n",
      "  [0.2 0.4]]\n",
      "\n",
      " [[0.  0. ]\n",
      "  [0.  0. ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.789866   1.57973199]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.95725846 0.04274154]\n",
      " [0.09374775 0.90625225]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.14324826 0.85675174]\n",
      " [0.57540819 0.42459181]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.        0.       ]\n",
      "  [0.        0.       ]]\n",
      "\n",
      " [[0.        0.       ]\n",
      "  [0.894933  2.0356678]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.85091398 0.14908602]\n",
      " [0.1895065  0.8104935 ]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[0.  0. ]\n",
      "  [1.5 0. ]]\n",
      "\n",
      " [[0.  1.5]\n",
      "  [0.  0. ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.773414   0.226586  ]\n",
      " [0.16215718 0.83784282]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.03176865 1.06353731]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.66666667 0.33333333]\n",
      " [0.25137692 0.74862308]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[0.53176865 0.56353731]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 5.90381802e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[1.12385753 1.59603577]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [1.06516009 2.08000349]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.49281471 0.50718529]\n",
      " [0.42388443 0.57611557]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[0.58773412 0.67997433]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[1.02896071 1.11197689]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[1.42606464 1.5007792 ]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[1.78345817 1.85070128]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[2.10511235 2.16563115]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[2.39460112 2.44906804]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[2.65514101 2.70416124]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[2.88962691 2.93374511]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.10066422 3.1403706 ]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.29059779 3.32633354]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.46153801 3.49370019]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.61538421 3.64433017]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.75384579 3.77989715]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 5.20950754e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.87846121 3.90190744]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.99061509 4.01171669]\n",
      "  [1.5        0.        ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[3.99061509 4.01171669]\n",
      "  [2.26270746 0.42541492]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [1.12864928e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.46542734 2.17018296]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.23866251e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[1.51147177 1.9364322 ]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[1.8603246  2.24278898]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[1.85622633 2.45339588]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.18256525e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.62882608 3.64468842]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[3.99061509 4.01171669]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.9752042  3.06644983]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[2.4796436  3.18457331]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.80627048 4.09617353]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[2.86232999 3.73075607]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.04806724 4.54038919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.07609699 3.85768046]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.26848729 3.97191242]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.35347054 4.30211189]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[0.3159464  0.6318928 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [3.04806724 4.54038919]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [3.04806724 4.54038919]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [3.62027965 4.87488127]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.09155358 4.11054502]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.18239822 4.19949052]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.2641584  4.27954147]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.33774256 4.35158732]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.40396831 4.41642859]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.46357147 4.47478573]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.51721433 4.52730716]\n",
      "  [2.81752467 1.08078065]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.51721433 4.52730716]\n",
      "  [3.29884401 1.76697026]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.82899032 4.25406967]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[3.35347054 4.30211189]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.51812349 4.3719007 ]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.70717361 4.63590286]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.50724419e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [3.79300927 5.29180178]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.87079051 4.93467214]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.26385778e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [3.94482084 5.61976975]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[3.98371146 4.94120493]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.06978407 5.21851036]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.47453865e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [4.10032405 5.89728902]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 5.64795997e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.16280566 5.19665933]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 5.77705336e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.22153245 5.45724527]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [4.238775   6.13154262]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 5.85855315e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.2993792  5.41152074]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 5.96658723e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.3451996  5.65837742]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [4.42627323 5.87669417]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.56549289 4.57457644]\n",
      "  [3.29884401 1.76697026]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.56549289 4.57457644]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.        0.       ]\n",
      "  [2.652575  5.3903858]]\n",
      "\n",
      " [[0.        0.       ]\n",
      "  [0.        0.       ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.03017581e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.44310909 5.67986638]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [4.49038025 6.21029364]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.51770665 5.82405064]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.23165443e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [1.21922689 1.81615568]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [4.57138728 5.9349774 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.60894361 4.6171188 ]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.64804924 4.65540692]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.68324432 4.68986623]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.71491989 4.7208796 ]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.7434279  4.74879164]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.76908511 4.77391248]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.7921766  4.79652123]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[0.         1.5       ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.7921766  4.79652123]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[1.80708266 4.57962026]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.26957742e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.58740824 5.78601628]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.34407427e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[1.21922689 1.81615568]\n",
      "  [4.57138728 5.9349774 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.57138728 5.9349774 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.70256428 5.88609719]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.81295894 4.81686911]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[1.80708266 4.57962026]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.83166304 4.8351822 ]\n",
      "  [3.88101813 2.585113  ]]\n",
      "\n",
      " [[1.80708266 4.57962026]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.83166304 4.8351822 ]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[1.80708266 4.57962026]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.16125079 6.00959941]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.62866741 5.70741465]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.66580067 5.63667319]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.47509917e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.71392605 5.67277547]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.78394736 5.87712147]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.83166304 4.8351822 ]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.77054197 5.68723632]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.63906063e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.8246389  5.87263362]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.84849674 4.85166398]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.86364707 4.86649758]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.87728236 4.87984782]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.38429448e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.88955412 4.89186304]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.90059871 4.90267674]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.40047345e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.91053884 4.91240906]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.91948496 4.92116816]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.92753646 4.92905134]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.93478281 4.93614621]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.42419344e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.94130453 4.94253159]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.94717408 4.94827843]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.43279161e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.95245667 4.95345058]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.43645714e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.957211   4.95810553]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.43975612e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9614899  4.96229497]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.44272521e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.96534091 4.96606548]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.96880682 4.96945893]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.97192614 4.97251304]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.97473353 4.97526173]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.97726017 4.97773556]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.45366803e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.97953416 4.979962  ]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.45524592e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98158074 4.9819658 ]\n",
      "  [4.50153907 3.44871082]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.98158074 4.9819658 ]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [3.50969342e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.48884218 6.27969424]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.62045692e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.79348777 5.61851269]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.81413899 5.55666142]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.83272509 5.50099528]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[1.49936142 2.03679115]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.70675334e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.83272509 5.50099528]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[2.68277075 3.21879369]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.70675334e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.84621811 5.59955109]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[2.68277075 3.21879369]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.69553149e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.85080669 6.17613724]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.8615963  5.53959598]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[2.68277075 3.21879369]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.8615963  5.53959598]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.28602389 3.82523524]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.87543667 5.48563638]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.28602389 3.82523524]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.87804101 5.71327309]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.28602389 3.82523524]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.87661975 6.37337786]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.8896684  5.90598769]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.28602389 3.82523524]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.93094217 6.17947525]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.45666602e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98342267 4.98376922]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.45794412e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9850804  4.9853923 ]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 3.4590944e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98657236 4.98685307]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98791512 4.98816776]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46106138e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98912361 4.98935099]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99021125 4.99041589]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99119013 4.9913743 ]\n",
      "  [5.01526985 4.12819518]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46333387e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.99119013 4.9913743 ]\n",
      "  [5.4031718  4.57597528]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.70028845 6.50224219]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.8896684  5.90598769]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.91721107 5.92478394]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.84305691e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.96194714 6.08628734]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99207111 4.99223687]\n",
      "  [5.4031718  4.57597528]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46394518e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.992864   4.99301318]\n",
      "  [5.4031718  4.57597528]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9935776  4.99371186]\n",
      "  [5.4031718  4.57597528]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46499052e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99421984 4.99434068]\n",
      "  [5.4031718  4.57597528]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.99421984 4.99434068]\n",
      "  [5.68170128 4.88888452]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.05407272e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.81702866 6.62103467]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.82400118e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.94338439 5.89690691]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.95832732 6.40190644]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.95502312 6.00921603]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.37621229e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.9768516  6.19868949]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99479786 4.99490661]\n",
      "  [5.68170128 4.88888452]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46583724e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.99479786 4.99490661]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.89052358 6.71420375]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.95952081 5.90829443]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.97050105 5.93362301]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.90676904e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.98634494 6.09730739]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99531807 4.99541595]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99578626 4.99587435]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99620764 4.99628692]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46681547e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99658687 4.99665823]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99692819 4.9969924 ]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99723537 4.99729316]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 3.4675286e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99751183 4.99756385]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99776065 4.99780746]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99798458 4.99802672]\n",
      "  [5.8676621  5.09285613]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46804848e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.99798458 4.99802672]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.15967443e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.93346221 6.73055108]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.89795588e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.97345094 5.84026071]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98126345 5.85905331]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.98567785 6.39227502]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9831371  5.77314798]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98583969 5.943484  ]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [2.43816836 3.2820688 ]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.99203276 6.1953482 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46804848e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99818612 4.99822404]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99836751 4.99840164]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46831419e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99853076 4.99856148]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99867769 4.99870533]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99880992 4.9988348 ]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99892892 4.99895132]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99903603 4.99905618]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99913243 4.99915057]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99921919 4.99923551]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99929727 4.99931196]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46895933e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99936754 4.99938076]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[2.78911175 6.05892032]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.99936754 4.99938076]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.91924253e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.98973295 5.94988128]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.92783713e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.99203276 6.1953482 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.92783713e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[2.43816836 3.2820688 ]\n",
      "  [4.99190956 6.47762661]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99075965 5.85489315]\n",
      "  [0.10609028 0.21218056]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.92607035e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99075965 5.85489315]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.215848   4.23208504]\n",
      "  [4.99190956 6.47762661]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.92766616e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99190956 6.47762661]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.92766616e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99225864 6.58077056]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99228328 6.0597548 ]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99587634 6.29013759]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99943079 4.99944269]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99948771 4.99949842]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99953894 4.99954858]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99958504 4.99959372]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46915902e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99962654 4.99963435]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46918781e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99966389 4.99967091]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46921373e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9996975  4.99970382]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.38889175 6.90685376]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46923705e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.9996975  4.99970382]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.69135919 7.3773288 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99449218 6.04593243]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99781717 6.14495032]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99972775 4.99973344]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.69135919 7.3773288 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.99972775 4.99973344]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99504296 5.94133919]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99664835 5.92864972]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99879968 6.07236854]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99975497 4.99976009]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99977948 4.99978409]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99980153 4.99980568]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46930923e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99982137 4.99982511]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46932301e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99983924 4.9998426 ]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99985531 4.99985834]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46934656e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99986978 4.9998725 ]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.84347647 7.60703737]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.99986978 4.9998725 ]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99698351 5.83578475]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.93470769e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99801163 5.84683979]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99860449 6.37492018]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99821047 5.76215581]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99838942 5.68594023]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99863651 5.89293819]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93695725e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99875685 6.54463537]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99882099 6.06432324]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93716869e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99890682 6.69804698]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9372577e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99897323 6.21138041]\n",
      "  [2.04980896 2.69714092]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99940132 6.34897249]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9998828  4.99988525]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99989452 4.99989673]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99990507 4.99990706]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99991456 4.99991635]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99992311 4.99992472]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999308  4.99993224]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99993772 4.99993902]\n",
      "  [5.99004048 5.23210956]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46940373e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.99993772 4.99993902]\n",
      "  [6.06840512 5.30827521]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.18351015e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.96632039 6.8498277 ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99897323 6.21138041]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99967575 6.17446185]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99994395 4.99994512]\n",
      "  [6.06840512 5.30827521]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46940806e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99994955 4.99995061]\n",
      "  [6.06840512 5.30827521]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46941195e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999546  4.99995555]\n",
      "  [6.06840512 5.30827521]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.9999546  4.99995555]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.92039757 7.67497858]\n",
      "  [0.         0.        ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98274949 6.90946602]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9990759  6.09024237]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99916831 5.98121813]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99945446 5.96039381]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99981971 6.08721314]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.9999546  4.99995555]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99965511 5.91508216]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93864371e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99989169 6.04358879]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46941545e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99995914 4.99995999]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99996322 4.99996399]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46942143e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999669  4.99996759]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46942398e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99997021 4.99997083]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46942628e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99997319 4.99997375]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99997587 4.99997637]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99997828 4.99997874]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46943188e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998046 4.99998086]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [0.         0.        ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46943339e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[4.99998046 4.99998086]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [4.42174018e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99123679 6.82076587]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99978423 5.8749766 ]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9387436e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99993803 6.02178674]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998241 4.99998278]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998417 4.9999845 ]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.95998057 7.72164681]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.99998417 4.9999845 ]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99980581 5.78747894]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99987812 5.80245417]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388079e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99992026 6.33187504]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99989031 5.72220875]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.93874167e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99991326 5.89385439]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.38775665e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99992543 6.52347927]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 6.93877352e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999268  6.0563189 ]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93879042e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99995638 6.26173344]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998575 4.99998605]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998718 4.99998744]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998846 4.9999887 ]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998961 4.99998983]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999065 4.99999085]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999159 4.99999176]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944111e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999243 4.99999259]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 3.4694417e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999319 4.99999333]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944222e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999387 4.99999399]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999448 4.9999946 ]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999503 4.99999514]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 3.4694435e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999553 4.99999562]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944385e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999598 4.99999606]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999638 4.99999645]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999674 4.99999681]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944469e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999707 4.99999713]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944492e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999736 4.99999741]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999762 4.99999767]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999786 4.99999791]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999808 4.99999812]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999827 4.9999983 ]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944575e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999844 4.99999847]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999986  4.99999863]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999874 4.99999876]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999886 4.99999889]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944616e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999898 4.999999  ]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999908 4.9999991 ]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999917 4.99999919]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944638e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999925 4.99999927]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999933 4.99999934]\n",
      "  [6.12073072 5.39406869]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944649e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.99999933 4.99999934]\n",
      "  [6.15686008 5.42534069]]\n",
      "\n",
      " [[3.97990398 7.71081404]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.21558149e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99558912 6.8329105 ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99994595 6.03285283]\n",
      "  [3.02466501 3.88815946]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93883337e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99995657 6.54400785]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99994595 6.03285283]\n",
      "  [3.51231089 4.35722086]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99995136 5.92956754]\n",
      "  [3.51231089 4.35722086]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99995622 5.83661079]\n",
      "  [3.51231089 4.35722086]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99996074 6.03590853]\n",
      "  [3.51231089 4.35722086]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.38776673e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.60468783 4.70709316]\n",
      "  [4.99997802 6.27200366]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[4.99999933 4.99999934]\n",
      "  [6.15686008 5.42534069]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99996074 6.03590853]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99997158 6.02675573]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388634e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99997802 6.27200366]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99998874 6.13600157]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999994  4.99999941]\n",
      "  [6.15686008 5.42534069]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999946 4.99999947]\n",
      "  [6.15686008 5.42534069]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999951 4.99999952]\n",
      "  [6.15686008 5.42534069]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944661e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999956 4.99999957]\n",
      "  [6.15686008 5.42534069]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999996  4.99999961]\n",
      "  [6.15686008 5.42534069]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944668e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.9999996  4.99999961]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.21799752e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99778319 6.82715754]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99997442 5.92408016]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.59887931 4.27501269]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99997442 5.92408016]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.79942942 4.50713841]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99997698 5.83167214]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.79942942 4.50713841]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998399 5.8702367 ]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.79942942 4.50713841]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99998796 6.41609546]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998559 5.78321303]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.79942942 4.50713841]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998703 5.70489173]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.79942942 4.50713841]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999887  5.91888405]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.79942942 4.50713841]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388772e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99998946 6.57560135]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.9999887  5.91888405]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999457 6.28780052]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999964 4.99999965]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 3.4694467e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999968 4.99999969]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999971 4.99999972]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999974 4.99999975]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944677e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999977 4.99999977]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999979 4.99999979]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999981 4.99999981]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999983 4.99999983]\n",
      "  [6.17666568 5.44583455]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[4.99999983 4.99999983]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.21921547e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99888707 6.78113239]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99998983 5.82699564]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999274 5.92861803]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93888637e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.21589728 4.11917368]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999722 6.14390019]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944683e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999985 4.99999985]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999986 4.99999986]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944686e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999988 4.99999988]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944687e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999989 4.99999989]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999999  4.9999999 ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999991 4.99999991]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944689e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999992 4.99999992]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 3.4694469e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999993 4.99999993]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999993 4.99999994]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999994 4.99999994]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944691e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999995 4.99999995]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999995 4.99999995]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999996 4.99999996]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999996 4.99999996]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944692e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999996 4.99999997]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944693e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999997 4.99999997]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999997 4.99999997]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999997 4.99999997]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 4.99999998]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944694e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 4.99999998]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 4.99999998]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 4.99999998]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944694e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 4.99999999]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.98993629 7.76977044]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999526 5.92186909]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999722 6.14390019]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889004e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999861 6.0719501 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18744612 5.45378029]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.21982825e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99944164 6.75931383]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999707 5.88971458]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889197e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999813 6.39186088]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999737 5.80074313]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999794 5.95711592]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999907 6.19593044]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99496524 7.75633243]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999814 5.86140432]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.9999987  5.90907434]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889261e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999953 6.09796522]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19327789 5.4393431 ]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.9997203  6.74328665]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999916 5.89372326]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889326e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999943 6.40647191]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999925 5.80435093]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999932 5.72391584]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999939 5.65152425]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999947 5.88835089]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889312e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999972 6.20323596]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99748179 7.76101258]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999962 5.92546983]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999971 6.47180591]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999969 6.05145728]\n",
      "  [3.75613974 4.59297384]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999973 6.65648587]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99999969 6.05145728]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999972 5.94631155]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999975 5.8516804 ]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999977 6.08843454]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889353e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999986 6.32824293]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1964156  5.42339708]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.9997203  6.74328665]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999979 5.97959109]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999981 5.88163198]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999983 5.79346878]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.8997105  4.88380975]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99999983 5.79346878]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.94985519 4.97320205]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889372e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999993 6.16412147]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99874068 7.73584665]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999989 5.86238298]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.94985519 4.97320205]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889381e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999997 6.08206073]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19809592 5.4090132 ]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22029078e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99986011 6.71659652]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999993 5.86401578]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.94985519 4.97320205]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999998 6.04103037]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99937027 7.68531084]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999994 5.7776142 ]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.94985519 4.97320205]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999996 5.80521925]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.94985519 4.97320205]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999999 6.02051518]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99968511 7.68826173]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 5.8108157 ]\n",
      "  [3.87806975 4.71706983]]\n",
      "\n",
      " [[3.94985519 4.97320205]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889389e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999999 6.33458387]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99999998 5.8108157 ]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.94985519 4.97320205]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889389e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999998 6.49161821]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[4.99999998 5.8108157 ]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 5.72973413]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999998 5.96151435]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.38777878e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.60794753 4.51714692]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999999 6.24580911]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19893608 5.40182126]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22036839e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99993005 6.742904  ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 5.86536291]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 5.9310051 ]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889389e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.80397376 4.7568971 ]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [4.99999999 6.24580911]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.93889389e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.80397376 4.7568971 ]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.12290455]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19941208 5.38754924]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22040722e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99996502 6.74385404]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 5.83790459]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 5.75411413]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 5.82621889]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.80397376 4.7568971 ]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.06145228]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19967806 5.39093622]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22042663e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99998251 6.70241457]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[4.99999999 5.743597  ]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79637941]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.80397376 4.7568971 ]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.3492779 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71674147]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.64506732]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.58056059]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.52250453]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80096343]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.3492779 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.38777878e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.49502432]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72086708]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95844327]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.24751216]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99984254 7.66621857]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86259894]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93030434]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.49587781]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.06350329]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.24793891]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99992127 7.71648659]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95715296]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97775205]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.97492759 4.8109273 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.80233512 4.86234805]\n",
      "  [5.         6.51507027]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.97775205]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.51507027]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.64863595]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.1483304 ]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.78365014]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.28762526]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.90687517]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.4065627 ]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.45343759]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [2.59309979 3.76378641]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22043634e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999125 6.91383237]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.26590643]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.21432825]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.71245009]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09289542]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.98360588]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.88524529]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79672076]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71704869]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.64534382]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00765195]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.75928582]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90688675]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81619808]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11181337]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.82436826]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00063203]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.98746379 5.01149176]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.00063203]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90056883]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.18003172]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.88419682]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.34369459]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.97957624]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.20932513]\n",
      "  [3.93903487 4.89236846]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.20932513]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.39649306]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.48978812]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19982504 5.39300972]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19990902 5.46203781]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999563 7.01551341]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.25684375]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.22433713]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.24489406]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19990902 5.46203781]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19990902 5.46203781]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19990902 5.46203781]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19990902 5.46203781]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19990902 5.46203781]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99996063 7.78364461]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [4.44088724e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999781 6.99749155]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.10190341]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99171307]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99381416]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.51997269]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89443274]\n",
      "  [3.96951743 4.92991428]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.89443274]\n",
      "  [3.98475872 5.07294622]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.90198688 4.91815971]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.25998635]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99998032 7.7495954 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80498947]\n",
      "  [3.98475872 5.07294622]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90648927]\n",
      "  [3.98475872 5.07294622]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.25998635]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.12999317]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99998032 7.7495954 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99998032 7.7495954 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99998032 7.7495954 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99998032 7.7495954 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99998032 7.7495954 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99998032 7.7495954 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999016 7.73739341]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81584035]\n",
      "  [3.98475872 5.07294622]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85991744]\n",
      "  [3.98475872 5.07294622]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.06499659]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999016 7.73739341]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999016 7.73739341]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999016 7.73739341]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999016 7.73739341]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999016 7.73739341]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999016 7.73739341]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85595736]\n",
      "  [3.98475872 5.07294622]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.03249829]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999508 7.71266368]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.840978  ]\n",
      "  [3.98475872 5.07294622]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.35264034]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.840978  ]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96154514]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.17632017]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19995276 5.53722427]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997551 5.56760876]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22044484e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999891 6.88336383]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95130064]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.08816009]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997551 5.56760876]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997551 5.56760876]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997551 5.56760876]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997551 5.56760876]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997551 5.56760876]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999754 7.69871478]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22044544e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999945 6.82220217]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91091435]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.95099344 4.96307439]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.40844578]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81982292]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.9937319  4.90599869]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.81982292]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97328977]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.40844578]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.5935388 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8759608 ]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78836472]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.70952824]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.63857542]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95670323]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.38777878e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.67945069]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.15013189]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.33972535]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11095608]\n",
      "  [3.99237936 4.87286431]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.61424511]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.11095608]\n",
      "  [3.99618968 4.88081459]]\n",
      "\n",
      " [[3.99686595 4.78092851]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.11095608]\n",
      "  [3.99618968 4.88081459]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.20117609]\n",
      "  [3.99618968 4.88081459]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.78759299]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08105848]\n",
      "  [3.99618968 4.88081459]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97295263]\n",
      "  [3.99618968 4.88081459]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.97295263]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.78297755]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.19966733]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.87135571]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.34837595]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.97502823]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.46419927]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         7.07319382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.56137716]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         7.16114778]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.64514769]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         7.23863297]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.71802703]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.61931648]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999877 7.80941015]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.54622433]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.42083876]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.30965824]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998732 5.53714991]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22044575e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999973 6.97943659]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [2.49999991 2.43537382]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.27875488]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99843297 4.83484669]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.27875488]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.16324074]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.62012542]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04691666]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.1715085 ]\n",
      "  [3.99809484 5.15544449]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.31006271]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999344 5.49745582]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999661 5.54050255]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22044590e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999973 6.97943659]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.1715085 ]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.97549672 5.04491551]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.15503135]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999661 5.54050255]]\n",
      "\n",
      " [[3.99999938 8.09191589]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999661 5.54050255]]\n",
      "\n",
      " [[3.99999969 8.01456134]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.05435765]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94892188]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93647348]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.15503135]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.45210507]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04907877]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.22605254]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999661 5.54050255]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0149604 ]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.51901043]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11508437]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.90116756 5.03720213]\n",
      "  [5.         6.25950521]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22044590e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999986 6.93575204]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00357593]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00559005]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.25950521]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.12975261]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999985 7.92691218]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999992 7.86569211]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95469607]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.44675473]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85922646]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00831512]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.22337737]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999992 7.86569211]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999992 7.86569211]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999992 7.86569211]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999992 7.86569211]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99350851]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.50909209]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89415766]\n",
      "  [3.99904742 5.10174733]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.89415766]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80474189]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00600778]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.65694915]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.16578355]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.794788  ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.30080698]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.91771679]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.4174902 ]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.45885839]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999996 7.8361721 ]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.27574118]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.22141395]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.71799478]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09927255]\n",
      "  [3.99952371 4.90853673]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.09927255]\n",
      "  [3.99976185 4.89397739]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.23683419]\n",
      "  [3.99976185 4.89397739]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.35899739]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1999982  5.56202591]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22044597e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999993 6.9626097 ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.16201605]\n",
      "  [3.99976185 4.89397739]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.17949869]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999998 7.98508213]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999999 7.95734749]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.16201605]\n",
      "  [3.99988093 4.91179511]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04581444]\n",
      "  [3.99988093 4.91179511]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9947067 ]\n",
      "  [3.99988093 4.91179511]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.08974935]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999999 7.95734749]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999999 7.95734749]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999999 7.95734749]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[3.99999999 7.95734749]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.87655642]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89523603]\n",
      "  [3.99988093 4.91179511]]\n",
      "\n",
      " [[3.99921649 4.9289253 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.89523603]\n",
      "  [3.99988093 4.91179511]]\n",
      "\n",
      " [[3.99960824 4.90036239]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.04487467]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80571243]\n",
      "  [3.99988093 4.91179511]]\n",
      "\n",
      " [[3.99960824 4.90036239]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.80571243]\n",
      "  [3.99994046 4.87384743]]\n",
      "\n",
      " [[3.99960824 4.90036239]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.02243734]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999904 5.55531377]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999997 6.80358982]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.80571243]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99960824 4.90036239]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72514118]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99960824 4.90036239]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.77154553]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99960824 4.90036239]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.01121867]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999949 5.56270076]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [2.22044603e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999998 6.71041312]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.77154553]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.00560933]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [3.8965429  5.56285903]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [3.99999998 6.71041312]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [5.55111510e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [3.99999999 6.66382477]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.7880165 ]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.00280467]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.79637262]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.70921485]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.63829336]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.57446403]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68835388]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.27674389]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.61951849]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8204568 ]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.98774836 4.9844703 ]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.46655466]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.73841112]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.66457001]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.59811301]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.88567837]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.46655466]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.58754868]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79711053]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03357474]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.29377434]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 5.50278631]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.74534228]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0342971 ]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.14688717]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.71339291]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7704153 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97590342]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.07344358]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7704153 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7704153 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7704153 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7704153 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.77556902]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87831308]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79048177]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71143359]\n",
      "  [3.99997023 4.75920868]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.71143359]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78509423]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.03672179]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.77556902]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.77556902]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80723583]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.34125523]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94012001]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.17062761]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 5.41692306]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [5.55111512e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.74871914]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84610801]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.76149721]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68534749]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81092479]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.40968372]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96933588]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.59257621]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.12169843]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.29628811]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.7018222 ]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.79959047]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07936446]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.14814405]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.79959047]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.79959047]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.83154102]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99893985]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.07407203]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.83154102]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.83154102]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.83154102]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.83154102]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.83154102]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.83154102]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.92909874]\n",
      "  [3.99998512 4.66417778]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.03703601]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.81534645]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.92909874]\n",
      "  [3.99999256 4.70372838]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87936377]\n",
      "  [3.99999256 4.70372838]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.37026352]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.7914274 ]\n",
      "  [3.99999256 4.70372838]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71228466]\n",
      "  [3.99999256 4.70372838]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.71228466]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.47004562]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.64105619]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99980412 4.85466866]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.64105619]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.57695057]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87649353]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.23502281]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.77931272]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.74025377]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78884418]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.70995976]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84898901]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.11751141]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.74025377]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.74025377]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.74025377]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.74025377]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.74025377]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.7640901 ]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68768109]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.61891298]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99990206 4.68375681]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.61891298]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.75646105]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.0587557 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.54827144 6.46559476]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.67694399]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68081495]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.61273345]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72986901]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.02937785]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [4.87413572 6.93228504]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [5.55111512e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.6304196 ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.65688211]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74019219]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.01468893]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 5.40659844]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 5.35546706]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.61128668]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.77597167]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.00734446]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 5.35546706]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 5.35546706]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.61603201]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79092362]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.32004168]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.92347848]\n",
      "  [3.99999628 4.8999696 ]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.95058378 5.02240315]\n",
      "  [5.         6.16002084]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.70972249]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.72425264]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.92347848]\n",
      "  [3.99999814 4.91399313]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.16002084]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.44940181]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83113063]\n",
      "  [3.99999814 4.91399313]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74801757]\n",
      "  [3.99999814 4.91399313]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.74801757]\n",
      "  [3.99999907 5.03675729]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.22470091]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.72425264]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.72425264]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.72425264]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.72425264]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.72425264]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.72425264]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.66133335]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86388915]\n",
      "  [3.99999907 5.03675729]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.45790611]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.77750023]\n",
      "  [3.99999907 5.03675729]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.77750023]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.22895306]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 5.3222482 ]]\n",
      "\n",
      " [[4.         7.66133335]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66133335]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [5.55111512e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.6190161 ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.69975021]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.62977519]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.56679767]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.5101179 ]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74664017]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.4131326 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.67197616]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.60477854]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.54430069]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.48987062]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99995103 4.5894436 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.48987062]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99997552 4.49067005]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81018835]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99997552 4.49067005]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.2065663 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66133335]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66133335]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66133335]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66133335]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.65474201]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72916951]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99997552 4.49067005]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84721128]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99997552 4.49067005]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.97529189 4.97520991]\n",
      "  [5.         6.10328315]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.65474201]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66625552]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.84721128]\n",
      "  [3.99999953 5.10154109]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.84721128]\n",
      "  [3.99999977 4.99208381]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.98764594 4.92891822]\n",
      "  [5.         6.10328315]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.98764594 4.92891822]\n",
      "  [5.         6.39052609]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.76249015]\n",
      "  [3.99999977 4.99208381]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68624113]\n",
      "  [3.99999977 4.99208381]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.899331  ]\n",
      "  [3.99999977 4.99208381]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.98764594 4.92891822]\n",
      "  [5.         6.55499544]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.899331  ]\n",
      "  [3.99999988 5.11804008]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.98764594 4.92891822]\n",
      "  [5.         6.63723012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8093979 ]\n",
      "  [3.99999988 5.11804008]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72845811]\n",
      "  [3.99999988 5.11804008]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.72845811]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0191211 ]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.63723012]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.7262635 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91720899]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.1491099 ]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.36313175]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66625552]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66625552]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.66625552]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11980765]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.18156588]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 5.3075369 ]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [5.55111512e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.75743111]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [3.74999995 3.68629031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03253017]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.50379501]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.92927716]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83634944]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01969272]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.2518975 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.30137489]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [5.55111512e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.75743111]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [4.37499998 4.25102224]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01060536]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.5301909 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90954483]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81859034]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.02137153]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.26509545]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        5.35365989]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12941177 4.50007695]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.12605041 0.87394959]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.75743111]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [4.68749999 4.53405973]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91923438]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99998776 4.58421953]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.91923438]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82731094]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74457985]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8783281 ]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99382297 5.11935116]\n",
      "  [5.         6.13254772]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12941177 4.50007695]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12941177 4.50007695]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12941177 4.50007695]]\n",
      "\n",
      " [[4.         7.79277172]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12941177 4.50007695]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89218314]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99691149 5.01269467]\n",
      "  [5.         6.13254772]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99691149 5.01269467]\n",
      "  [5.         6.42314712]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80296483]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72266834]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93059302]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99691149 5.01269467]\n",
      "  [5.         6.58381077]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83753372]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.05229117]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99691149 5.01269467]\n",
      "  [5.         6.29190538]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12941177 4.50007695]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12941177 4.50007695]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.20912547 0.79087453]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.75743111]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [4.84374999 4.68794633]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94706205]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85235584]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94294008]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99691149 5.01269467]\n",
      "  [5.         6.52312872]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08072153]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99691149 5.01269467]\n",
      "  [5.         6.69385297]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.21790195]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99691149 5.01269467]\n",
      "  [5.         6.83408727]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09611176]\n",
      "  [3.99999994 4.85040328]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.09611176]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.28169078]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999388 4.65980352]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.83408727]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.92971995]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.28169078]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.97753629]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.43185991]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.48876814]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.04759562 3.99368543]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.24500907 0.75499093]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [4.84374999 4.68794633]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.28867392]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.23984422]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.74032176]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11585979]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.2540586 ]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.37016088]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.7477171 ]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.12865274]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01578747]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91420872]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00516871]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999694 5.10178974]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.58714792]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.00516871]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90465184]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08718509]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.728448  ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.23497174]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.8582127 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.36077095]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.42910635]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.98659273 3.7085971 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.24500907 0.75499093]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[2.         3.04430838]\n",
      "  [4.84374999 4.68794633]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.25202801]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.21455317]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.95609128 3.5855849 ]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.75077291 3.38110493]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.58441559 0.41558441]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[2.         3.04430838]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.12682521]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04923388]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999847 4.95296235]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.52697014]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.04923388]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94431049]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84987944]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03572777]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.67777618]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.932155  ]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8389395 ]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09058022]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99387418 5.07885701]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.33888809]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.75077291 3.38110493]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.75077291 3.38110493]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.75077291 3.38110493]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.75077291 3.38110493]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.60574949 0.39425051]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.         4.45838628]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9815222 ]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.02631633]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.33888809]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.16944404]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.87548199]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9236847 ]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83131623]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999923 4.89617473]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.83131623]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74818461]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84186992]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.08472202]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.63616674 3.27386475]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.16333634e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.5        5.06594111]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.75768293]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81273027]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.04236101]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.84826753]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82330954]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.02118051]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.51808337 3.42028689]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.65904169 3.73651989]]\n",
      "\n",
      " [[4.         7.74922587]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.75       5.36229437]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74097859]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.66688073]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74191257]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.30735528]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.66772131]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.60094918]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.54085426]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79336924]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.47102534]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71403232]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94542629]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.61368319]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85088366]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07091511]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.30684159]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.65904169 3.73651989]]\n",
      "\n",
      " [[4.         7.80297898]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9638236 ]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86744124]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78069711]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91308519]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.1534208 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.65904169 3.73651989]]\n",
      "\n",
      " [[4.         7.80297898]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.65904169 3.73651989]]\n",
      "\n",
      " [[4.         7.80297898]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.65904169 3.73651989]]\n",
      "\n",
      " [[4.         7.80297898]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.65904169 3.73651989]]\n",
      "\n",
      " [[4.         7.80297898]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.65904169 3.73651989]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82177667]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.73959901]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.66563911]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79418787]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99845574 5.23998224]\n",
      "  [5.         6.39438555]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71476908]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.64329218]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.57896296]\n",
      "  [3.99999997 4.86364634]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.57896296]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8472357 ]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99922787 5.17774534]\n",
      "  [5.         6.39438555]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99922787 5.17774534]\n",
      "  [5.         6.53608705]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.76251213]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68626092]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95756528]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99922787 5.17774534]\n",
      "  [5.         6.65106964]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.13921049]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99922787 5.17774534]\n",
      "  [5.         6.32553482]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.875      5.63683138]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.02528945]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04285865]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99961394 5.1190866 ]\n",
      "  [5.         6.32553482]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99961394 5.1190866 ]\n",
      "  [5.         6.57991087]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 6.9388939e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.15339367]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99961394 5.1190866 ]\n",
      "  [5.         6.7513129 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03805431]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93424887]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84082399]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.75674159]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999962 4.78061386]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.75674159]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.7513129 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.67835309]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68106743]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.61296069]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97782158]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.73030518]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.88003942]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79203548]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08813981]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.36515259]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.82952084 4.01317769]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.9375     5.75367162]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09013094]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.18257629]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [5.96476042 4.2613214 ]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.96875    5.81288818]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01809599]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.09128815]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.76672357]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94556325]\n",
      "  [3.99999999 4.66340836]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.04564407]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.79060018]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.94556325]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.99999981 5.09083209]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.94556325]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.40104734]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85100693]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9859224 ]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.20052367]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.03706786 7.13692012]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.984375   5.80081305]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.88733016]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79859714]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87950804]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.10026183]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.05738021 4.43212935]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.9921875  5.75220974]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79155724]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71240151]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.64116136]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.76068541]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.05013092]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.12244011 4.53638989]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99609375 5.68037904]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68461687]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.7623608 ]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.02506546]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.15809505 4.56907884]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 5.54569411e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99804687 5.64513384]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68612472]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.75308854]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.31376815]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.67777969]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8643971 ]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.15688407]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.10603393 6.89361533]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[2.59999999 2.68416525]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99902344 5.66832576]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.77795739]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.70016165]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81283446]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.40357582]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96784756]\n",
      "  [3.99999999 4.70992948]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.20178791]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77352539]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.96784756]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96463894]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.48674953]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86817505]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.02878734]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.9999999  4.96367367]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.6548897 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.02878734]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.99999995 5.14379272]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.32744485]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.77390172]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04537161]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.99999995 5.14379272]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.58187107]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.15543423]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.99999995 5.14379272]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.29093553]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.17748503 4.55669104]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18835189 4.54567582]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99902344 5.66832576]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 5.54975987e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99951172 5.79633657]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09409133]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.99999995 5.14379272]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.5831043 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.18028738]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.99999995 5.14379272]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.29155215]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18835189 4.54567582]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18835189 4.54567582]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18835189 4.54567582]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.18835189 4.54567582]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99975586 5.87028324]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.18028738]\n",
      "  [4.         4.74210376]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.14577607]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19398063 4.59137254]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99987793 5.90725657]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.18028738]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.07288804]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.79846579]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01929891]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.44416358]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08731489]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.22208179]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19689266 4.64379957]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99993896 5.88855424]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03249016]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.1110409 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.1983975  4.68480241]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.87134785]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99996948 5.85727318]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96066144]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.05552045]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9025389 ]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.02776022]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.8199385 ]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81228501]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999998 5.08851722]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.81228501]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81724659]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.01388011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.77098481]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.73552193]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.66196974]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.59577277]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.53619549]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.67364979]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.00694006]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.71239104]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.60628481]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.54565633]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.67560419]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.2737117 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.60804377]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81350657]\n",
      "  [4.         4.82936231]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.46225848]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.81350657]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.73215591]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95098135]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.23112924]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19917434 4.6978229 ]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99998474 5.80902913]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85588321]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.77029489]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87759914]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.46660428]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78983923]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 4.86917261]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.78983923]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.23330214]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62565544]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.88824047]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.11665107]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.62876341]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19957496 4.69182073]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89078066]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.4146378 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8017026 ]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96670642]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.2073189 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19978138 4.66952201]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999237 5.79119713]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96628077]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.10365945]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19978138 4.66952201]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19978138 4.66952201]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19978138 4.66952201]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999619 5.78211087]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.92460416]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.42167139]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03097064]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.62322395]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.1647749 ]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.31161198]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19988764 4.65123986]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999809 5.8569654 ]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04829741]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94346767]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99637862]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.15580599]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19994229 4.63846428]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999905 5.82703415]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89674076]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91069278]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99693709 5.07498374]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.07790299]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19997038 4.6620183 ]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 5.5511138e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999952 5.77779418]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8196235 ]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84097295]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.07790299]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.0389515 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19998481 4.66182281]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999976 5.72528627]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83606707]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.01947575]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66967789]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82582384]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.34006741]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74324145]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.66891731]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87048562]\n",
      "  [4.         4.74008378]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.1700337 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999221 4.64202908]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999988 5.71083738]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.87048562]\n",
      "  [4.         4.83805537]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.4332111 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78343706]\n",
      "  [4.         4.83805537]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96500297]\n",
      "  [4.         4.83805537]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.60260674]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.96500297]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86850267]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07529403]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.30130337]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999601 4.61112905]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999796 4.58989948]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999994 5.7855363 ]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96776463]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87098816]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78388935]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.70550041]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.63495037]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83799653]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[3.99999999 5.02122802]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.15065168]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999796 4.58989948]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999796 4.58989948]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999796 4.58989948]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999896 4.60916426]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999997 5.72796676]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.83799653]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.07532584]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999947 4.59576884]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999999 5.699182  ]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.75419688]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80722878]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.36055443]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.7265059 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.65385531]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87114943]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.52873699]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04706951]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.26436849]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999947 4.59576884]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999947 4.59576884]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999947 4.59576884]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999947 4.59576884]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999947 4.59576884]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999947 4.59576884]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[3.99999999 5.7684188 ]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94236256]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8481263 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.76331367]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.6869823 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.61828407]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81488943]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.45814002]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99070073]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.22907001]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999973 4.57755722]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.78048969]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.98697837]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99980697 5.26006846]\n",
      "  [5.         6.11453501]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.66926577]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93930319]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.97087468]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99990348 5.07584823]\n",
      "  [5.         6.11453501]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.11453501]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.43298878]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.93930319]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0428471 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.63363323]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93856239]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.12273449]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.76591041]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.26773141]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.89004777]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.38988481]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.44502388]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999986 4.59614613]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.94619877]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.25089633]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.1258067 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01322603]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91190342]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03396127]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99846854 4.96865307]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.63609645]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93056514]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83750863]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07319289]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.63609645]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.31804822]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.72942423]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9658736 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01015609]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.56308655]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90914048]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07980486]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.28154327]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.15223571 6.7048612 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.90502133]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97182437]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.86115862]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.97182437]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.81930906]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9985295 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.81930906]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.54018344]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89867655]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.81930906]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80880889]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         4.81930906]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.80880889]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.59361527]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04185056]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.71354786]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.20634442]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.8393117 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.33889689]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         6.95521461]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.2050072 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.38458944]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         7.03144308]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.50487195]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         7.11767032]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.35438476]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.52426051]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99995174 4.98373812]\n",
      "  [5.         7.16853936]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.37183446]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.55333297]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99997587 5.3592848 ]\n",
      "  [5.         7.16853936]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99997587 5.3592848 ]\n",
      "  [5.         6.58426968]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         7.79398927]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.41037436]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99997587 5.3592848 ]\n",
      "  [5.         6.85628458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.26933692]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.3771823 ]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99997587 5.3592848 ]\n",
      "  [5.         6.97901521]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.48019723]\n",
      "  [4.         4.80502887]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99997587 5.3592848 ]\n",
      "  [5.         6.48950761]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.01832783]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.48019723]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.33217751]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.19895976]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07906378]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.13533493]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99997587 5.3592848 ]\n",
      "  [5.         6.2447538 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999993 4.61026894]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 4.667143  ]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.90664464]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.06556899]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99997587 5.3592848 ]\n",
      "  [5.         6.5486045 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95901209]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09894784]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.5486045 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.71388139]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.98905306]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89014775]\n",
      "  [4.         4.99459333]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.89014775]\n",
      "  [4.         5.18284922]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99923427 5.13876511]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.71299979]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.13027379]\n",
      "  [4.         5.18284922]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.71299979]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.3564999 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 4.667143  ]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 4.667143  ]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 4.667143  ]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 4.667143  ]]\n",
      "\n",
      " [[4.         8.10124281]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 4.667143  ]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.10773686]\n",
      "  [4.         5.18284922]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.62134469]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.2024063 ]\n",
      "  [4.         5.18284922]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.31067235]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999996 4.667143  ]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.93428484]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08216567]\n",
      "  [4.         5.18284922]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.06535178]\n",
      "  [4.         5.18284922]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.15533617]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         8.00273092]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9588166 ]\n",
      "  [4.         5.18284922]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.9588166 ]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86293494]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89360194]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.43510886]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80424174]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97616442]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.6080202 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87854798]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08248207]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.3040101 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999998 4.69622936]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.90013525]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97423386]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         5.0257279 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.97423386]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87681047]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78912943]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71021648]\n",
      "  [4.         4.97495125]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.71021648]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.15200505]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.92750617]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.63919484]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.57527535]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.51774782]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71967593]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.07600252]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.74783968]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79023897]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.03800126]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.66179021]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.71121508]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.77080804]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.32732385]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.69372724]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87779316]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.51477919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79001384]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0009186 ]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.65775703]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.16356211]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.79430336]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0472059 ]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.9025575 ]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.0472059 ]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.87016111]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.24132429]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.87016111]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99961714 5.25458247]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.8936814 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.37813471]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.87016111]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.8936814 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.99809458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.24032124]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.87016111]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11628911]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         4.87016111]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.11628911]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         5.23431839]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.49904729]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.64699069]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.77001099]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.11628911]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0046602 ]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90419418]\n",
      "  [4.         5.00907966]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.90419418]\n",
      "  [4.         4.8662175 ]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.05171601]\n",
      "  [4.         4.8662175 ]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.24952365]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.77001099]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.77001099]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.77001099]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.77001099]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.77001099]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94654441]\n",
      "  [4.         4.8662175 ]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97308166]\n",
      "  [4.         4.8662175 ]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.12476182]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.19999999 4.72182861]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.83930029]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93644556]\n",
      "  [4.         4.8662175 ]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.43695914]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04300643]\n",
      "  [4.         4.8662175 ]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.63568214]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93870579]\n",
      "  [4.         4.8662175 ]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.93870579]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.69332339]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.14668225]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.34666169]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72096841]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69620432]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.87832304]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03201402]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.92881262]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00307099]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.57455924]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.13135919]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.7398233 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.26160891]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99980857 5.3847638 ]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.87455521]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.38062654]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99990428 5.44220398]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.87455521]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99990428 5.44220398]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.98952822]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.24256389]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.41709323]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99990428 5.44220398]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.49476411]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69620432]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69620432]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         6.00599882]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.27538391]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.2355976 ]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99990428 5.44220398]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.7416211 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.31444724]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.7416211 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.37081055]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.8056919 ]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.20554784]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.18540527]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.92862484]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08499305]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.01665864]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.49936609]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91499277]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.05724282]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.24968305]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.94653156]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.02849463]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.53623937]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.12874307]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.26811969]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.17611786 6.73091011]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.95449663]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07161941]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.56270761]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.16089275]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.2813538 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69943138]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.94160542]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04480347]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03494326]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.1406769 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.73151434]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 5.55111512e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.88478001]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97374239]\n",
      "  [4.         5.08738161]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.07033845]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.89616291]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.97374239]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87636815]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78873134]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82250105]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.03516923]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.83757841]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82531821]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.06367484]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.3477119 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.82531821]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.50398323]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74278639]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97298649]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.25199162]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.74239934]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72511167]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.8315846 ]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.98728989]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.52091177]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8885609 ]\n",
      "  [4.         4.93318776]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.8885609 ]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.61588024]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.09063255]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.74419314]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.24299353]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.37209657]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72511167]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72511167]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72511167]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72511167]]\n",
      "\n",
      " [[4.         7.74778962]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.72511167]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11869418]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00682476]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.05225101]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.60694869]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94702591]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.11629243]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.74999132]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00466319]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.20232812]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.37499566]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.69518968]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.89672355]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.08209531]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.07092218]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.08209531]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.08545935]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99998794 5.2990842 ]\n",
      "  [5.         6.62033595]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.18918203]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.08545935]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99999397 5.29767648]\n",
      "  [5.         6.62033595]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.62033595]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.78584079]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07026383]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.08545935]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.96323745]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.08545935]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8669137 ]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.08545935]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.8669137 ]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.39292039]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.70628426]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[3.89960937 3.60941293]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.79512726]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.78022233]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94727932]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.57537193]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85255139]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.05642447]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.71025575]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.21231453]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99995214 5.41775043]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.84005369]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.34217874]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.84005369]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.42002684]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.70628426]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[4.54980469 4.12275737]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.79512726]\n",
      "  [4.921875   4.84478437]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.62641009 0.37358991]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[4.54980469 4.12275737]\n",
      "  [4.         6.95145952]]\n",
      "\n",
      " [[4.         5.79512726]\n",
      "  [4.9609375  4.95926368]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.23910011]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.21001342]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.2        4.67119303]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.63606981 0.36393019]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[4.54980469 4.12275737]\n",
      "  [4.         6.9713698 ]]\n",
      "\n",
      " [[4.         5.79512726]\n",
      "  [4.9609375  4.95926368]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.10355542]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999698 5.29697262]\n",
      "  [5.         6.10500671]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.33988567 4.48680585]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.63606981 0.36393019]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[4.54980469 4.12275737]\n",
      "  [4.         6.9713698 ]]\n",
      "\n",
      " [[4.         5.8389858 ]\n",
      "  [4.9609375  4.95926368]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.9937804 ]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.10500671]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.05250336]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.87109222]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91789154]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.02625168]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.4098285  4.39557839]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.44479991 4.35740388]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.63606981 0.36393019]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[4.54980469 4.12275737]\n",
      "  [4.         6.85284152]]\n",
      "\n",
      " [[4.         5.8389858 ]\n",
      "  [4.9609375  4.95926368]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82610239]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74349215]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.66914293]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74507214]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.01312584]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.63606981 0.36393019]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.01478801 4.2152419 ]\n",
      "  [4.         6.85284152]]\n",
      "\n",
      " [[4.         5.8389858 ]\n",
      "  [4.9609375  4.95926368]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.48637197 0.51362803]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.01478801 4.2152419 ]\n",
      "  [4.         6.85284152]]\n",
      "\n",
      " [[4.         5.71752175]\n",
      "  [4.9609375  4.95926368]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.7777864 ]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.00656292]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.46228562 4.33256516]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.52856859 4.32976329]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.48637197 0.51362803]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.01478801 4.2152419 ]\n",
      "  [4.         6.85284152]]\n",
      "\n",
      " [[4.         5.71752175]\n",
      "  [4.98046875 4.7907464 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79151837]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [0.         0.        ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.00328146]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.52856859 4.32976329]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.52856859 4.32976329]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.52856859 4.32976329]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.56375094 4.30475536]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.49139979 0.50860021]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.01478801 4.2152419 ]\n",
      "  [4.         6.85284152]]\n",
      "\n",
      " [[4.         5.71752175]\n",
      "  [4.99023437 4.71198055]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79151837]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.25706599]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.31824808]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.79151837]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         4.94514034]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.92305842]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         4.94514034]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.52834741]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83075257]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         4.94514034]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74767732]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         4.94514034]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.98517762]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         4.94514034]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.65824475]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.88665986]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         4.94514034]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.88665986]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.68378632]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79799387]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.07251146]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.34189316]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.56375094 4.30475536]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.56375094 4.30475536]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.49387697 0.50612303]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.30786617 4.23660498]\n",
      "  [4.         6.85284152]]\n",
      "\n",
      " [[4.         5.71752175]\n",
      "  [4.99023437 4.71198055]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.43089254 0.56910746]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.30786617 4.23660498]\n",
      "  [4.         6.85542534]]\n",
      "\n",
      " [[4.         5.71752175]\n",
      "  [4.99023437 4.71198055]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.073013  ]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.17094658]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.58234764 4.2813617 ]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.43089254 0.56910746]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.30786617 4.23660498]\n",
      "  [4.         6.85542534]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99023437 4.71198055]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00488513]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.08547329]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.61659373 4.27634974]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.43089254 0.56910746]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.30786617 4.23660498]\n",
      "  [4.         6.85542534]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99511719 4.75794433]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90439662]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81395696]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84116779]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.37920376]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.75705101]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68134591]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89235446]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.18960188]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.83305827]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.77347092]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80311902]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72280711]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.6505264 ]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.58547376]\n",
      "  [4.         5.07495859]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.58547376]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.09480094]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.77347092]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.77347092]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.77347092]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.77347092]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.77347092]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.73065726]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.33966337]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90119398]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.16983169]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.63371677 4.28193815]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.43209916 0.56790084]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.47998424 4.26895142]\n",
      "  [4.         6.85542534]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99511719 4.75794433]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.40205107 0.59794893]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.57800382 4.28909732]\n",
      "  [4.         6.85542534]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99511719 4.75794433]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.38673548 0.61326452]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.57800382 4.28909732]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99511719 4.75794433]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91852966]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.08491584]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.62092496]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.67787435]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.8266767 ]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.13586807]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.8266767 ]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.04245792]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.67787435]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.67787435]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.67787435]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.67787435]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.67787435]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83032152]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.35335757]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.95650379]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.17667878]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.64290954 4.291618  ]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.38673548 0.61326452]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.63310994 4.29578638]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99511719 4.75794433]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37862679 0.62137321]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.63310994 4.29578638]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99755859 4.76157368]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86085341]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.77476807]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85805555]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.08833939]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.66960785]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.86436353]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.38991511]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.98814781]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.19495755]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.6655628  4.29704672]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37920345 0.62079655]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.66426734 4.30051086]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.99755859 4.76157368]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37476477 0.62523523]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.66426734 4.30051086]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78796608]\n",
      "  [4.9987793  4.77604596]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.97205693]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.00190037]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.48630155]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.97205693]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.09547081]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.24315077]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68049377 4.30114103]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68992118 4.30607967]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37505137 0.62494863]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.66426734 4.30051086]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78280581]\n",
      "  [4.9987793  4.77604596]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87485123]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.09547081]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.93468593]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.09547081]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999849 5.090489  ]\n",
      "  [5.         6.12157539]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68992118 4.30607967]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68992118 4.30607967]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68992118 4.30607967]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68992118 4.30607967]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37505137 0.62494863]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.66426734 4.30051086]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78280581]\n",
      "  [4.99938965 4.76189735]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.91597312]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.09547081]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.12157539]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.06078769]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.67802615]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.82437581]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.09547081]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74193823]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.09547081]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.6677444 ]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         5.09547081]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.6677444 ]\n",
      "  [4.         5.01332005]]\n",
      "\n",
      " [[4.         4.81483316]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.6677444 ]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.81483316]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.29749161]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.60096996]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.81483316]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.81948162]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.81483316]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.47653845]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00035619]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.81483316]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.6384117 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.00035619]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.80755906]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.15554278]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.80755906]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.31920585]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69463489 4.30803289]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37519457 0.62480543]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.66426734 4.30051086]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78280581]\n",
      "  [4.99969482 4.84316579]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.0399885 ]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.80755906]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.04767659]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         4.80755906]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.57867356]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         6.04767659]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999925 4.99387465]\n",
      "  [5.         6.28933678]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.70540232]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03957301]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.28933678]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99997607 5.44489669]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.56049759]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.14398554]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99998804 5.34664738]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.56049759]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99998804 5.34664738]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.73784301]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.26712998]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99998804 5.34664738]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.36892151]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69708708 4.30664308]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37526615 0.62473385]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.66426734 4.30051086]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78280581]\n",
      "  [4.99984741 4.92843488]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.14041698]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.02637528]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.06075624]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99998804 5.34664738]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.60876325]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.17388342]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.60876325]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.30438163]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69836082 4.3194829 ]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37530193 0.62469807]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.68197477 4.33061853]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78280581]\n",
      "  [4.99984741 4.92843488]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.3728239  0.6271761 ]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.69181954 4.34742638]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78280581]\n",
      "  [4.99984741 4.92843488]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.3714603  0.6285397 ]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.78280581]\n",
      "  [4.99984741 4.92843488]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37070726 0.62929274]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.86095627]\n",
      "  [4.99984741 4.92843488]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.10869436]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.15219081]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.7717718 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.82936364]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99782492]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89804243]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90989754]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.44005442]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.03097054]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.63241543]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.92787349]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83508614]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.75157752]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.67641977]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99117606]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.31620771]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.82936364]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.82936364]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89205845]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.80285261]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.72256735]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.65031061]\n",
      "  [4.         4.9309751 ]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.65031061]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.85163839]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.49875921]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.76647455]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.6898271 ]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.94441723]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.24937961]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.81115224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84997551]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.76497796]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.68848016]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84399192]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.46228657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.00691059]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.23114329]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.69902151 4.34010455]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37070726 0.62929274]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.86095627]\n",
      "  [4.99992371 4.86698168]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99591261]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.51393669]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         6.10353098]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.25696834]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70118944 4.36140034]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37072506 0.62927494]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.78819026]]\n",
      "\n",
      " [[4.         5.86095627]\n",
      "  [4.99996185 4.87490323]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99317788]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.99937628]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.12848417]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70228549 4.36173464]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37073396 0.62926604]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[4.         5.86095627]\n",
      "  [4.99996185 4.87490323]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.89943865]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.90111299]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.06424209]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.78334302]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.75211671]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.87625333]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.03524895]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999402 5.31682899]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.38262238]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.87625333]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.38262238]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.19131119]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.75211671]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.788628  ]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.7097652 ]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.83140707]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.09565559]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70283956 4.36323053]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.7031166  4.36445297]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37073396 0.62926604]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[4.         5.86095627]\n",
      "  [4.99998093 4.77001445]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.74826637]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.67343973]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.60609576]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.54548618]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.49093756]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.44184381]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.65918414]\n",
      "  [4.         4.72561179]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.0478278 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.7031166  4.36445297]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.7031166  4.36445297]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.7031166  4.36445297]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.7031166  4.36445297]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.7031166  4.36445297]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70325813 4.34746326]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37073841 0.62926159]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[4.         5.86095627]\n",
      "  [4.99999046 4.64868088]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.65918414]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.28758755]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.84462709]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         5.07067343]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.14379378]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70325813 4.34746326]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70325813 4.34746326]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70325813 4.34746326]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70325813 4.34746326]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70325813 4.34746326]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37074063 0.62925937]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[4.         5.86095627]\n",
      "  [4.99999523 4.66219128]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.84462709]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         4.99285422]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.40974773]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.98621264]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         4.99285422]\n",
      "  [3.         3.40131258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.20487386]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.72655969]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.88759137]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         4.99285422]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         4.99285422]\n",
      "  [3.         3.40131258]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         4.99285422]\n",
      "  [4.5        5.18260584]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999962 5.01267204]\n",
      "  [5.         6.10243693]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70333041 4.31860825]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.37074175 0.62925825]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[3.8        5.50352047]\n",
      "  [4.99999523 4.66219128]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [6.24500451e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         4.99285422]\n",
      "  [5.25       6.03227769]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.         6.10243693]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [6.9388939e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.1        6.46412954]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.         4.99285422]\n",
      "  [5.665      6.60179066]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.07767178e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.05       6.23206477]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.         7.7577649 ]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.266      8.01959871]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [5.665      6.60179066]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.291      6.75674865]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.0985     6.9416116 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.23168445e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.48865    7.24745044]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.360725   7.32642468]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.68979    7.3089442 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.41363959e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.7246525  7.59378221]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.63824225 7.72046878]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.344895   6.6544721 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.266      8.01959871]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.266      8.01959871]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.266      8.01959871]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.266      8.01959871]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.266      8.01959871]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.266      8.01959871]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.45707912 7.52202323]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.1724475  6.32723605]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.70336731 4.30644782]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.34514937 0.65485063]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.69728729 4.35679891]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[3.8        5.50352047]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [8.9609974e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.81137121 7.76982091]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.47466461 7.41580487]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.17821289e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.67608959 7.12993997]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.82719815 7.67422439]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.68403491 7.68908818]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.01165876 7.64060526]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.63798091e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.74668096 7.9007862 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.25142787e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.50582938 6.82030263]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.68601071 4.26760411]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.45482076 0.54517924]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.75742824 4.38027513]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[3.8        5.50352047]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.44709439 0.55290561]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 2\n",
      "Reward: [2 0]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [4.         6.79384564]]\n",
      "\n",
      " [[3.8        5.50352047]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.4426457  0.5573543 ]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [4.69867238 7.5572373 ]]\n",
      "\n",
      " [[3.8        5.50352047]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.68145034e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.07201286 8.11070758]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.73833818 7.78347484]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.64087318e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.25291469 6.41015132]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.75178995 4.33567773]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.30730136 0.69269864]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.04467147 7.89200859]]\n",
      "\n",
      " [[3.8        5.50352047]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.67566137e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.06450437 8.00512736]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.80396925e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.63341806 7.5666242 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.12645735 6.20507566]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [6.88988034 4.483309  ]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.46944695e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.02645111 4.62805564]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.21261257 0.78738743]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.17570296 7.97265397]]\n",
      "\n",
      " [[3.8        5.50352047]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.20571683e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.97007625 7.80996178]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.27306863 8.02896561]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.54576176 8.22606904]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.23592403e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [6.82346382 7.59506479]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.11438872e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.7926142  7.14056374]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.14111744 7.83555831]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.95514563e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.42700569 8.05200248]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.03070409e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.68430513 8.24680223]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.15919824 7.97962661]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.3963071  6.57028187]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.02645111 4.62805564]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.02645111 4.62805564]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.02645111 4.62805564]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.12539128 4.72752191]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.16962642 0.83037358]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.17570296 7.97265397]]\n",
      "\n",
      " [[4.7636793  6.44361088]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.44327842 8.18166395]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.69895058 8.36349756]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.         4.78193702]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.92905552 8.5271478 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.5018875e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.92905552 8.5271478 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.36977576 7.69600006]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.5018875e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [8.13614997 8.67443302]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.64558814e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.61598529 8.41561653]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.73128199 8.21424664]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.85438676 8.57405488]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.45007564e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.61970618 8.5727261 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.34153032e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.91352347 8.53621376]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.28723327e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.57526248 8.70084855]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.79722058e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.98686672 8.7484463 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.81773623 8.8307637 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.7036148  8.91476037]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [7.07487928 8.9401273 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.34545658e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [4.15852284 5.01908126]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.93325332 9.02328433]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.50480031e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.93325332 9.02328433]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.50480031e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [8.13992799 9.1209559 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.89991571 9.13652887]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.90918367e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [7.19740593 9.1246752 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.4816677e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [7.82892022 9.21813451]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.99420361e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.09870296 7.5623376 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.20296057 4.79899698]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.28342421 0.71657579]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.71941957 8.67358079]]\n",
      "\n",
      " [[4.7636793  6.44361088]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [8.0460282  9.29632106]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.11661072e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.02       4.98925302]\n",
      "  [8.24142538 9.36668895]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.71863763e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [8.24142538 9.36668895]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.23182528e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.84592163 8.52784438]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.71863763e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [7.85908134 9.09448223]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.75031239e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [7.06659335 8.90171508]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.63599995e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [8.07317321 9.18503401]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.60188924e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [7.86322395 9.15320304]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.90343416e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [7.17858626 9.11213876]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.45620767e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [7.80304648 9.22145702]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.98114484e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.08929313 7.55606938]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.20296057 4.79899698]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.38924811 5.86893081]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [7.93728911e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.98092837 9.0253732 ]]\n",
      "\n",
      " [[4.7636793  6.44361088]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [7.33724049 8.63315626]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.2252959e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.47954276 8.23129719]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.09123333e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [7.26043735 8.60909701]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.99217195e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.73977138 7.1156486 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.38924811 5.86893081]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.38924811 5.86893081]]\n",
      "\n",
      " [[4.7882969  8.59798687]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.38924811 5.86893081]]\n",
      "\n",
      " [[5.29832339 9.24263224]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.03794045e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [7.53439361 8.74818731]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [7.78095425 8.87336858]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.3991216e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [8.00285883 8.98603172]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.11061977e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.44948119 5.51956155]\n",
      "  [8.20257294 9.08742855]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [8.20257294 9.08742855]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.19482994e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [3.99999701 5.21146345]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.48921505 8.25487719]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.67645039e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[5.         5.        ]\n",
      "  [7.38924811 5.86893081]]\n",
      "\n",
      " [[5.93019087 9.75628754]\n",
      "  [5.18805893 6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.13833567e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [8.38231565 9.17868569]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.8163999e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [8.54408408 9.26081712]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.46772806 8.93235944]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [5.48921505 8.25487719]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.80890808e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[3.99999981 4.94731079]\n",
      "  [6.23169875 8.70038237]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.18177727e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.72095525 9.0391235 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.35748893e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.35315713 8.9997147 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[4.4926794  5.95380834]\n",
      "  [6.23169875 8.70038237]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.32410964e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[4.4926794  5.95380834]\n",
      "  [6.55711222 8.95007706]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.61784141 9.09974323]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.28593933e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.4317656  9.12990244]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[4.86918459 6.556935  ]\n",
      "  [6.55711222 8.95007706]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.5499106e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[5.05743719 6.85849833]\n",
      "  [6.55711222 8.95007706]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.5499106e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[5.05743719 6.85849833]\n",
      "  [6.15063246 9.37755355]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.11489653e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.93780167 10.03010474]\n",
      "  [ 5.18805893  6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.68858904 9.2169122 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.33503036e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.3045475  9.35947752]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.26785861e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[5.05743719 6.85849833]\n",
      "  [5.9504369  9.70081867]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.89071984 10.25884338]\n",
      "  [ 5.18805893  6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0137096e-16 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.57409275 9.42352977]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.2555826e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.16722114 9.59209235]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.12894503e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[5.05743719 6.85849833]\n",
      "  [6.3421069  9.68724628]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.25256275 6.11885436]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.12045333 9.67094469]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.80144139e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[5.05743719 6.85849833]\n",
      "  [6.02734139 9.94716049]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.79354125 10.49779956]\n",
      "  [ 5.18805893  6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.47446271 6.92780505]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.12045333 9.67094469]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.47446271 6.92780505]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.408408   9.70385022]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.14061571e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.47446271 6.92780505]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.6675672  9.7334652 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.32044353e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.47446271 6.92780505]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.90081048 9.76011868]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.09645771e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.47446271 6.92780505]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.36134179 9.85892353]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.18230824e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.19568452 5.9076826 ]]\n",
      "\n",
      " [[5.05743719 6.85849833]\n",
      "  [6.45820741 9.91714966]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.47446271 6.92780505]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.62520761 9.87303118]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.         5.79883224]\n",
      "  [5.47446271 6.92780505]]\n",
      "\n",
      " [[4.52064914 5.60604021]\n",
      "  [7.39588677 9.90337545]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.4812816e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.04652021 10.15769465]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.85512533 10.71024996]\n",
      "  [ 5.18805893  6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.02638547e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.47446271  6.92780505]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.11655147 10.01476559]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.19561622e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.36988069 10.08475356]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.47446271  6.92780505]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.10622801 10.04128422]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.52743155 10.05889047]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.47446271  6.92780505]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.16408662 10.0441983 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.5293155e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.62935042 10.04712455]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.9710837e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.47446271  6.92780505]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.23378348 10.04094897]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.70818861 10.03994186]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.01944561e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.47446271  6.92780505]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.30016718 10.03645123]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.6547409e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.77416118 10.03455142]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.47446271  6.92780505]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.35974806 10.03204619]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.40103714e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.22913072 10.3013757 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.87146189 10.86794346]\n",
      "  [ 5.18805893  6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.47446271  6.92780505]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.62377326 10.02884157]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.29005538e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.78674066  7.47543915]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.62377326 10.02884157]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 5.78674066  7.47543915]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.86139593 10.02595741]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.45493923e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.86139593 10.02595741]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.45493923e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.42235025 10.13352898]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.32232772e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.58350546 10.20409944]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.15029009e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.68011523 10.12017609]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.32915047e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.91210371 10.10815848]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.58945404 10.13571902]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.13644918e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.14033749 10.4492271 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.97151256 10.98825933]\n",
      "  [ 5.18805893  6.76785458]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.26624164e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.83050863 10.12214711]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.43350686e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.37138931 10.2407644 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.26071504e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.51872447 10.32091931]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.11492884e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.29318444 10.24874992]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.52327375e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.14796726 10.55576339]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.97151256 10.98825933]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [4.15010274e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.90773796 9.61218657]]\n",
      "\n",
      " [[4.7636793  6.44361088]\n",
      "  [5.58282927 5.33990493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.52064914  5.60604021]\n",
      "  [ 7.10577913 10.34668032]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.53201851e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 5.96258865 10.67318543]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.82806793 11.13280179]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.86124949e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.10577913 10.34668032]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 5.8125215  10.78971343]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.75634562 11.20507302]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.93062475e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.          5.79883224]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 6.87789816 10.48922553]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.000000e+00 1.000000e+00]\n",
      " [4.033247e-17 1.000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 5.708799   10.87688593]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.62933207 11.29822672]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.54500113e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 6.87789816 10.48922553]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 6.72246868 10.59536714]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.04338697 10.67658982]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.66464969e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.05022181 10.53583042]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.34519963 10.48224738]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.09675609e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.0899546  10.51175962]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [8.3868842e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.35767533 10.54299876]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.38095914 10.46058366]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.53647077e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.64286323 10.41452529]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.30330171e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.36450174 10.42446215]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.62463836 10.44128424]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.11014963e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.62805157 10.38201593]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.29302405e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.86524641 10.34381434]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.07872177 10.30943291]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.60573933e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.68921623 10.33123015]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.19353255e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.88800567 10.35313418]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.33546556e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.92029461 10.29810713]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.09916168e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.12826515 10.26829642]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.64011695e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.31543863 10.24146678]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.76999464e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.48389477 10.2173201 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.88688457e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.63550529 10.19558809]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.9920855e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.07295491 10.23904772]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.19573567 10.69585778]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 3.9061338e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 6.043848   11.24473245]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.60173776e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.51477172 10.39786697]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.29915524e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.60377652 10.50707568]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.39889647 10.40176376]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.58229047e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.76144685 10.41424334]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.54020473e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.40402698 10.36657921]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.69169623e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.84233421 10.35375336]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.13757576e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.66362428 10.32992129]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.56874582 10.30646199]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.74782312e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.94866544 10.29946147]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.25187243e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 6.0379287   7.74810254]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.81187124 10.27581579]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.42057457e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.81187124 10.27581579]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.82160522e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 7.09908122 10.26005705]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.62617237e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.03068412 10.24823421]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.22761571 10.22341079]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.70905525e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.95344034 10.21572822]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.46707981 10.62792151]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.19376201e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 6.20330014 11.20865751]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.51880787e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.56355209 10.35903271]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.48743807e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.21485996 10.79742376]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 6.12707091 11.24794184]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 7.80719688 10.32312944]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.0264772  10.2908165 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.11389747e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 4.64536003  7.07229428]\n",
      "  [ 8.22382948 10.26173485]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 8.22382948 10.26173485]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.70642802e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.59785872 10.44983693]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.64657347 10.57864665]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.05441471e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.45755875 10.45637712]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.22397362e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.80631023 10.47187417]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.71180287 10.41073941]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.35113819e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.57842553 10.39411937]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.35398348 10.73511382]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 6.09490566 11.28161867]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.25858907e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.82058298 10.35470744]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.42661955e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 8.03852468 10.31923669]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.57784699e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.56085573 10.45366388]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.11495401 10.88020438]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 6.07179512 11.32227488]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.24639757e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.80477016 10.40829749]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.41564721e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.34836668 10.5562305 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.24310171e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 5.98619505 10.96901214]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.21315422e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.97524423 11.38362964]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.06866136 10.66572011]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.15375724e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.32056207 10.75079411]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.80973825e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.36179523 10.59914809]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.1082716e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.20912244 10.59989169]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.38577096e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.54393001 10.61535373]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.00233358e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.22213323 10.54608734]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 5.05743719  6.85849833]\n",
      "  [ 6.1620627  10.86112872]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.87647541 11.41024976]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.01136162e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.79843052  7.99383586]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.07589169 10.61749516]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.27578993e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 4.99354367  7.77370065]\n",
      "  [ 6.1620627  10.86112872]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.27578993e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 4.99354367  7.77370065]\n",
      "  [ 5.93162151 10.99466426]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.76859438 11.45212294]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.90988617e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.07589169 10.61749516]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.90988617e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.36830252 10.55574564]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.63147227 10.50017108]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.29539764e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.18838474 10.64795125]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.11588924e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.93162151 10.99466426]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.11588924e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 6.34116465 10.75651263]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.13065823 10.62658067]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.40006688e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.97802008 10.95910549]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.00276644e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.73656048 11.47669374]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.89577619e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 6.95653715 10.69693253]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.19568452  5.9076826 ]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.78363423 11.07023024]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.          5.        ]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.6508951  11.51711988]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.82706732e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 6.79172227 10.77655836]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.02640486e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.41129595  7.3819334 ]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.78363423 11.07023024]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.01320243e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.41129595  7.3819334 ]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.65217516 11.14196307]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.6508951  11.51711988]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.84219231e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.54213646 11.56918329]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.71270402e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 6.6567312  10.84506441]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.84396875e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.41129595  7.3819334 ]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.98878006 10.9090073 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.61903515e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 6.99105808 10.76055797]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.29195227 10.68450217]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.05980831e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.04148816 10.70585401]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.31110189e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.98878006 10.9090073 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.15555094e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.71124461 11.08217697]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.58766349 11.56693325]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.88601392e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 6.80524192 10.78579779]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.5906877  11.16786178]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.51592851 11.59778574]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.72208517e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.12471773 10.70721801]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.94376604e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.41224596 10.63649621]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.14327883e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 6.94239806 10.78539282]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.87931888e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 6.07230307 10.89808802]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.72957194  8.24391599]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.24815825 10.70685354]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.24815825 10.70685354]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.02942011e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.52334243 10.63616818]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.22036749e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.61221181  7.64084108]\n",
      "  [ 7.77100819 10.57255136]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.39222013e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.77100819 10.57255136]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.74252294 11.0881583 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.82744427e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.86636753 11.52791342]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.39222013e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.99390737 10.51529623]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.54688751e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.29396286 10.69291144]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 6.28884662 10.82124373]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.56456657 10.62362029]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.24897249e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.80810992 10.56125826]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.41959361 10.60912662]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 6.61226075 10.65427251]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.3547011  10.56627232]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 6.15267739 10.93830162]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.87506421 11.49046563]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.10334906e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.13842151 10.65845681]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.26927756e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.92636438 11.06533706]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.79290071 11.50861554]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.90654989e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.42457936 10.59261113]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.03036737e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.08283543 10.72244039]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.11224136e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.86942044  8.28471603]\n",
      "  [ 5.78034247 11.13611475]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.01963234e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.72958452 11.54328393]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.8535547  10.81566609]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.01091831e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.78034247 11.13611475]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.01091831e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.68200505 11.18537094]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.60621414 11.5979084 ]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.69957937 10.88198142]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.94268302e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.58348818 11.23184883]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.89009251e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.48293882 11.65174677]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.29753409e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.58318496 10.93373025]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.87432321e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.48491962 11.27662312]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.80455307e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.37474339 11.69936548]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.5680022e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.48556033 10.97751437]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.61185506e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.39235717 11.31805776]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.72947742e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.28159583 11.74068849]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.5002615e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.39972303 11.01598029]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.74169943e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.30881691 11.35530427]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.26035804  7.60684795]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.20068712 11.77673636]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.44069991e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.32338828 11.05011185]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.68373173e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.68376377 11.09769688]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.38773204e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.69104945 10.94510067]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.64284823e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.76404927  8.40469941]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.61903023 10.91162909]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.94390338e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.60116    8.05456962]]\n",
      "\n",
      " [[4.74684721 8.59680392]\n",
      "  [5.4460251  9.59158762]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.23432224  7.34616316]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.20068712 11.77673636]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.21089001  7.11154684]\n",
      "  [ 7.38924811  5.86893081]]\n",
      "\n",
      " [[ 5.20068712 11.77673636]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [3 0]\n",
      "QTable: [[[ 5.21089001  7.11154684]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.20068712 11.77673636]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[1.00000000e+00 1.11022302e-16]\n",
      " [4.09931669e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.90773796 9.61218657]]\n",
      "\n",
      " [[4.7636793  6.44361088]\n",
      "  [5.93902673 7.0346041 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.61903023 10.91162909]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.18574971e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.95712721 10.82046618]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.26141449 10.73841956]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.03861847e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.53527304 10.6645776 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.78174574 10.59811984]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.06928291 10.13569497]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.60116    8.05456962]]\n",
      "\n",
      " [[4.74684721 8.59680392]\n",
      "  [5.30736856 8.64041255]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.20068712 11.77673636]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.42805673 11.44264617]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.90530041e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.75115926 7.59510633]\n",
      "  [5.52963673 8.56700134]]\n",
      "\n",
      " [[5.23502713 8.17965575]\n",
      "  [6.65758888 9.5240125 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.60116    8.05456962]]\n",
      "\n",
      " [[4.74684721 8.59680392]\n",
      "  [5.81671983 9.12981128]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.75115926 7.59510633]\n",
      "  [5.52963673 8.56700134]]\n",
      "\n",
      " [[5.23502713 8.17965575]\n",
      "  [6.65548237 9.41393076]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.03616018e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[0.         0.        ]\n",
      "  [4.60116    8.05456962]]\n",
      "\n",
      " [[4.74684721 8.59680392]\n",
      "  [6.07055286 9.33047794]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.23633721e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.75115926 7.59510633]\n",
      "  [5.52963673 8.56700134]]\n",
      "\n",
      " [[5.23502713 8.17965575]\n",
      "  [6.75596233 9.43915656]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.21229222e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.70649912 10.24229744]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.76647097e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.41641329 10.99698571]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.68789058e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.75115926 7.59510633]\n",
      "  [5.52963673 8.56700134]]\n",
      "\n",
      " [[5.23502713 8.17965575]\n",
      "  [6.66058081 9.81649725]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [3.9596792e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 6.01748189 10.04774762]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.62170636e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[5.75115926 7.59510633]\n",
      "  [5.52963673 8.56700134]]\n",
      "\n",
      " [[5.23502713 8.17965575]\n",
      "  [6.73728316 9.92734768]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.67530626 10.42266809]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.40311991 10.96943192]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.67492931e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.63876408 10.13274108]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.18141044e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.74684721  8.59680392]\n",
      "  [ 5.4989011  10.59910682]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.49833516e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.35706559 11.03781239]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.97488768 10.11946697]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.27739891 10.10752027]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.54965902 10.09676824]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.97438995 10.28802685]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.81562913e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.57298404  8.53804468]\n",
      "  [ 5.4989011  10.59910682]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.57298404  8.53804468]\n",
      "  [ 5.39227678 10.71467836]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.46828877 11.13411694]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.83945519e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.27695095 10.25922416]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.04939906e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.54925586 10.23330175]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.23834854e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.93153864 10.40252222]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.74164365e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 5.39227678 10.71467836]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [7.4832873e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 5.96875385 10.51834807]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.85327086 10.40860034]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 5.67169243 10.71282081]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.79438756e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.47545273 11.2304986 ]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.16794377 10.3677403 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.97376014e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.85264886 10.46899848]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.93552721e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 6.07690576 10.54400979]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.75498034e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.85708673 10.45210316]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 5.72863397 10.76420434]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.48056106 11.29609056]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.71999696 10.53173331]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.97503834e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 5.55654141 10.90053839]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.80290317e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.42827931 11.36073861]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.66293459e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.58261504 10.62608201]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 5.44958243 10.99456464]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.76662542e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.34718567 11.43080211]\n",
      "  [ 5.98640081  7.99407657]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.92435354 10.56347381]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.60947091e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.23191818 10.50712643]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00363026e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.50872637 10.45641379]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.21022556e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.52963673  8.56700134]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.75785373 10.41077241]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.61492772e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.75785373 10.41077241]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.38309239e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.98206836 10.36969517]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.53867255e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.17086715 10.58267344]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.78140743e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.44340273  8.55489369]\n",
      "  [ 6.09313808 10.7303517 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.49273659e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.02268881 10.5834774 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.6589566   8.56958752]\n",
      "  [ 6.09313808 10.7303517 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.22796386e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.6589566   8.56958752]\n",
      "  [ 5.68544331 10.93749669]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.42071081e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.34718567 11.43080211]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.0295005  0.9704995 ]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.90773796 9.61218657]]\n",
      "\n",
      " [[4.7636793  6.44361088]\n",
      "  [6.27858889 7.75069301]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.87296925e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.78552173 10.66673738]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.6589566   8.56958752]\n",
      "  [ 6.05693034 10.7354433 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.41680307e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.10696955 10.60006364]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.86290154e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.97625691 10.59420914]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 6.05693034 10.7354433 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.2028397e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 5.66733944 10.94004249]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.71035541e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.4640956  11.45308471]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.84075066e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.75506423 10.67312157]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 5.51930796 11.05125513]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.79147797e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.43407349 11.49579098]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.07955781 10.60580941]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.91243005e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.74750209 10.72340676]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.82978924e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 5.43328338 11.12394396]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.77064594e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.41603758 11.53725819]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.68202011e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.5470644  10.81128096]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.77009769e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 5.38305672 11.17687526]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.51626203e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.32684455 11.59314148]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.42675489 10.87639058]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [7.4704919e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 5.76223032 10.93899386]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.45945703e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.51826957 10.81379284]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.99835048e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 5.51185298 11.10675352]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.2707301  11.62208787]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.46387598 10.84959783]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [3.8246163e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.75225044  8.57897108]\n",
      "  [ 5.84147688 10.89321589]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.34556449e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.81748838 10.76463804]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.74533494 10.73960538]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.05333883e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.84147688 10.89321589]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.11887242 10.7424501 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.82021644 10.66678273]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.24582065e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.28752278 10.63793814]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.13819479 10.60010446]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.42437531 10.54009401]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.22719677 10.52522226]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.36284535e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.75205343 10.96780422]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.52624376 11.52113284]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.91441976 10.64973282]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.99128885e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.58652422 11.09235525]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.83460191e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.52888978 11.52045955]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.69181957 10.76180851]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.96998994 10.85090103]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.73390576 10.72126466]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.69655088 11.03363433]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.45800719 11.54873564]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.67258576e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.64557323 10.77408607]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.60116     8.05456962]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.00650473 10.82645159]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.72538851 10.71762367]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.33569982e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.00650473 10.82645159]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.33569982e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.68645524 11.03272005]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.78725329e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.419159   11.56141729]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.66667573e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.63727635 10.77189986]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.94577096e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.99813816 10.82511997]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.21107128e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.97354872 10.69470987]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.67774294e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.27619385 10.62523888]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00977474e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.03735219 10.64266743]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.16204443e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.31400996 10.66962696]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.76628804e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.33361697 10.57840069]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.08871901e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.60025527 10.52056062]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.2737365e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.84022975 10.46850456]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.44025224e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 8.05620677 10.4216541 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.5901164e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.55370737 10.47867783]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.76244904e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.67848793 10.52628461]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.04828748e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.79833663 10.43081005]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.41118305e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 8.01850297 10.38772904]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 8.21665267 10.34895614]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 8.3949874  10.31406053]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.16503854e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.86888887 10.36754411]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.26826383e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.98679951 10.41015995]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.4601385e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.72916424 10.34783603]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.84806605e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.16106336 10.82964689]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.8012452  11.41984306]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.36318506e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.95624782 10.31305243]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.52075595e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.44254925 10.48838497]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.55019299e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.55755138 10.61017743]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.03286119e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.69829433 10.43954647]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.06835295e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.92846489 10.39559183]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.50147767e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.587253   10.44186689]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.81367689 10.48183547]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.51909725 10.41366763]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.7279381e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.91447735 10.40638479]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.21742181e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.52533957 10.36938773]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.27773675 10.77112962]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.91075842 11.35767662]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.04435066e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.77280561 10.33244896]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.39346735e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.99552505 10.29920406]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000e+00 1.00000e+00]\n",
      " [5.54801e-17 1.00000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 8.19597254 10.26928366]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.68709839e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.60908097 10.44309367]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.35605493e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.00317175 10.92863546]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.10141256e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.9990116  11.35607578]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.05597211e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.20580918 10.59300102]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.16553718e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.90119051 11.00674804]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.88182947 11.4152383 ]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.00003454e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.48522827 10.53370092]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.19392048e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.10309034 10.66954968]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.80332705 11.06946934]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.78215087 11.47543902]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.85751805e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.86795986  8.44780963]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.3927813  10.60259471]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0259545e-16 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.3927813  10.60259471]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.65350317 10.54233524]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.06213693e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.14808241 10.69895535]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.05373413e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.26089649 10.81431681]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.95997854e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.0783998  10.6752044 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.34436965e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.94330859 10.99733401]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.01217314e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.72243536 11.50780127]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.91162652e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.37055982 10.60768396]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.06260335 10.70277559]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.12399878e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 5.76062844 11.10178751]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.97073718e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.68625902 11.53501087]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.80133106e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.83555305 10.7921028 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.99723896e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.71271597  8.6467719 ]\n",
      "  [ 6.11453544 10.86773488]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.74311774e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.8635907  10.74314535]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.80217216  8.6704799 ]\n",
      "  [ 6.11453544 10.86773488]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.24281127e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.11453544 10.86773488]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.83177133 11.04787179]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.58856579 11.56476357]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.17723163 10.66883082]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.96040976e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.45950847 10.60194773]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.03521476e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.06246276 10.72012258]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [8.0932085e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.24087077 10.81198493]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.80113596e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.35621649 10.64811032]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.10440057e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.17445655 10.64884913]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.8558617  11.03189789]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.87784651e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.66406551 11.54192144]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.97827928e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.92957295 10.73718372]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.12664061e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.69355706 11.13271752]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.60386194 11.56583421]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.61671431e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.7422093  10.82167887]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.90139767e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.04366225 10.89503031]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.6783475e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.89109245  8.4649427 ]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.06798837 10.73951098]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.80880428e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.06798837 10.73951098]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.95145908 10.72776762]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.19363311e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.7633759  11.07384884]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.88846034e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.5825146  11.57402415]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.82354371e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.7810799  10.79342334]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.61469379 11.16653408]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.50368926 11.60438141]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.41159582e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.63641747 10.8633253 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.16879294e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.96191388 10.92859716]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.70297429 10.80310152]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.13690879e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.68243265 11.10605115]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.81895159e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.43303435 11.62343131]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.30224548e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.6244602  10.84397122]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.94297972e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.70318189  8.35786545]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.9910004  10.89061406]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.59664265e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.96201418 10.75957409]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.66173555e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.26581276 10.68361669]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.04167039e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.53923149 10.61525502]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 7.1660159  10.66387313]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.15709162e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.9910004  10.89061406]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.15709162e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.66871394 11.09467955]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.76992489e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.58292353 11.57726491]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.97242241e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.85049353 10.76980839]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.93346046e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.07455438 10.85526313]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.85506852 10.72700945]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.7704466  11.05853753]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.74786282e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.18980101  6.90039216]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.53348917 11.57943623]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.23502713  8.17965575]\n",
      "  [ 6.7357129  10.78691974]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.00405168e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.59861897 11.16104326]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.53348917 11.57943623]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.83962943e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.46102975 11.60448601]\n",
      "  [ 6.35494176  8.12938939]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.67383972e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.7357129  10.78691974]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.06214161 10.70822776]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.77051839 10.81853118]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.88482231e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.00751684 10.9079341 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.09346655 10.73667807]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.3841199  10.66301026]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.12376246e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.64570791 10.59670923]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.3052756e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.88113712 10.53703831]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.46863743e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.3435753  10.6316928 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [8.3371044e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.68817032 11.09576146]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.46102975 11.60448601]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.26485941 0.73514059]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.90773796 9.61218657]]\n",
      "\n",
      " [[5.31926977 7.97448256]\n",
      "  [6.27858889 7.75069301]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.09562899e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.60921777 10.56852352]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.07987701 10.72256634]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.94696104e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.52849706 11.18967513]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 3.7893506e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.56246568 11.59126954]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.82530309e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.37188931 10.65030971]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.02305516e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.89734348 10.80102491]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.67233091e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.02318592 10.91524753]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.20760913 10.72092242]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.00128351e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.48684822 10.64883017]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.19504455e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.7381634  10.58394716]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.36942948e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.27835607 10.65807259]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.35884961e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.73657923 11.09413158]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.71947184e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.69257527 11.55886381]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.55052046 10.59226533]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.23922604e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.79546841 10.5330388 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.08183857e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 8.01592157 10.47973492]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.30259248 10.67752009]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.98055147e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.64531972 11.17061131]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.76732462 11.55043994]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.06719144e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.57233323 10.60976808]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.05087234e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.81509991 10.54879127]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.42281491e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 8.03358992 10.49391215]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.57442281e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.27492285 10.7152006 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.91722746e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.62958971 11.20548163]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.79363145 11.56130021]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00959836e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.54743056 10.64368054]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0474164e-16 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.77274157  8.52827574]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.02555117 10.80403292]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.90631257e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.12501532 10.92435399]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.87495542e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.02555117 10.80403292]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.87495542e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.96278171 10.77175806]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [8.5001663e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.87996024 11.08669708]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.68192841 11.58935333]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.83337495 10.82055786]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.16008405e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.71275148 11.17908987]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.94262984e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.57431419 11.62289981]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.15003746 10.73850207]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.96133513e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.86011932 10.84088698]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.96401765e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.10042347 10.92589973]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.76016401e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.17410739 10.75679829]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.45669665 10.68111846]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.17412269e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.71102699 10.61300661]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.35059981e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.93992429 10.55170595]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.50942922e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.41013153 10.64621287]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.51426435 10.72143501]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.14181165e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.66911838 10.58159158]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.32151988e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.44026493 10.57936979]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.52017892e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.73323815 10.59246542]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.1627209e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.41342772 10.52667107]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.67212251e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.09634475 10.94539263]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.73591494e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.75252818 11.52211833]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.67208495 10.47400396]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.32357835e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.90487646 10.42660356]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.4851099e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.39097613 10.59145884]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.50456282 10.70927985]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.12851992e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.29731319 10.57944136]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.02689427e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.05329269 10.96348726]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.79518937 11.49283571]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.01270564e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.56758187 10.52149722]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.20510801 10.64614351]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.40868955 10.74020103]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.69659125  8.58575104]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.16602982 10.61915217]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 4.74799111  8.53517835]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.5707567  10.61776139]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.71470756  8.54053639]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.16602982 10.61915217]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.94486413e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.71470756  8.54053639]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.21131759 10.55668064]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.55937836e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.5707567  10.61776139]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.36781351e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.1034541  10.90601498]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.02122042e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.78212172 11.46909011]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.71470756  8.54053639]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.04704044 10.64074631]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.86457574 11.04064353]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.01215292e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.70987704 11.49084358]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.88986659e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.71470756  8.54053639]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.86935051 10.73663057]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.18002807 10.81497399]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.32936501  8.59625779]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 6.86935051 10.73663057]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.28825591e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 6.33775424 10.70213922]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.76656944e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.32936501  8.59625779]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.18241546 10.66296751]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.98380189e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.53764869  8.5633159 ]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.18241546 10.66296751]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.98380189e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.53764869  8.5633159 ]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.46417392 10.59667076]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.17931109e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.53764869  8.5633159 ]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.71775652 10.53700368]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.53764869  8.5633159 ]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.94598087 10.48330332]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.10272636e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.53764869  8.5633159 ]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.50809213 10.52250735]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.39770043e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.84690026  8.6823339 ]\n",
      "  [ 5.95282794 10.94740704]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.85817537 11.45442473]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.20978547e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.50809213 10.52250735]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.13060415e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.95282794 10.94740704]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.13060415e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.47965082 10.68270646]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.75728292 10.47025661]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.47050179 10.50821089]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.72802613 10.54463759]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.18370193e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.42646134 10.47196048]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.33701189e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.8345976  10.46110299]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.15314273e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.31179873  8.40459577]\n",
      "  [ 7.68381521 10.42476443]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.72942545  8.37220366]\n",
      "  [ 7.68381521 10.42476443]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.33171785e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.72942545  8.37220366]\n",
      "  [ 7.57574665 10.39682341]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.26056895 10.81232138]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.95938634 11.38594173]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.25673022e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.72942545  8.37220366]\n",
      "  [ 7.2921009  10.52334026]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.34414237e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.54712484 10.6154968 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.05991145e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.72942545  8.37220366]\n",
      "  [ 7.26490039 10.50786885]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.08596092e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.15731695 10.86212509]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.88565333 11.3961184 ]\n",
      "  [ 6.52841121  8.13663081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.72942545  8.37220366]\n",
      "  [ 7.53841035 10.45708196]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.72942545  8.37220366]\n",
      "  [ 7.78456931 10.41137377]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.40163005e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.78456931 10.41137377]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.40163005e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.35521144 10.55053692]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.27249691e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.93291981 10.98950991]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.88565333 11.3961184 ]\n",
      "  [ 6.60715019  8.13758782]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.43908998 0.56091002]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.90773796 9.61218657]]\n",
      "\n",
      " [[5.31926977 7.97448256]\n",
      "  [6.58137902 8.09556127]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.10370318e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.05077364 10.67107242]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.11679011e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.82072123 11.05320231]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.76313612 11.46648817]\n",
      "  [ 6.60715019  8.13758782]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.89245702e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.34569628 10.60396518]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.09710071e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 5.14995552  8.66062077]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.61112665 10.54356866]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.28128003e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.61112665 10.54356866]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.03893671e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.71561506 11.11319643]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.92601872 11.45067155]\n",
      "  [ 6.60715019  8.13758782]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.28128003e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.09180935 10.7170629 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.96600465e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.72821502 11.13686683]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.92601872 11.45067155]\n",
      "  [ 6.63972668  8.0819145 ]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.58711246 0.41288754]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[5.79300864 4.39558621]\n",
      "  [5.90773796 9.61218657]]\n",
      "\n",
      " [[5.49635862 8.77406644]\n",
      "  [6.58137902 8.09556127]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 6.83719068 10.81327818]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.97474763e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.734515   11.14870204]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.11200152e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.69788563 11.55064705]\n",
      "  [ 6.63972668  8.0819145 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.74425408e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 6.71240134 10.86611991]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.19373573e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.05221804 10.92079898]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.65766407e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.04116121 10.77950792]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.88578706e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 6.94146782 10.75807355]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.30269614 10.76362891]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 6.99181237 10.68448834]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.37337399e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.44807302 10.65560979]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.85154442e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.29263113 10.61603951]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.22554477 10.57026367]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.47424946e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.61425442 10.55591036]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00274577e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.5029903  10.5132373 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.04124907e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.75269127 10.46191357]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.37951022e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.5220474  10.45332093]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.58956097e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.00229824  8.51469373]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.81594617 10.45928355]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.76984266 10.40798884]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.39141139e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.6112998  10.38770784]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.45902546e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.81594617 10.45928355]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.18712734 10.8499006 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.89346274 11.43040666]\n",
      "  [ 6.63972668  8.0819145 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.05628004e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.85016982 10.34893706]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.44714955e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.39993584 10.51442877]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.29318202e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.95094876 10.99711296]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.30829618  8.58197057]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.90670571 11.42097484]\n",
      "  [ 6.63972668  8.0819145 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 5.97854045  8.35065133]\n",
      "  [ 7.65994226 10.46298589]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 6.05324713  8.36052002]\n",
      "  [ 7.65994226 10.46298589]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.31515267e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.90326625  8.75159131]]\n",
      "\n",
      " [[ 6.05324713  8.36052002]\n",
      "  [ 7.89394803 10.4166873 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.47752679e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 5.75115926  7.59510633]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 6.05324713  8.36052002]\n",
      "  [ 7.89394803 10.4166873 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.83815666 11.06694642]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.09860042e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.90670571 11.42097484]\n",
      "  [ 6.63972668  8.0819145 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.11093207 11.37716234]\n",
      "  [ 6.63972668  8.0819145 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.47752679e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 6.05324713  8.36052002]\n",
      "  [ 7.89394803 10.4166873 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.09550536e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 6.05324713  8.36052002]\n",
      "  [ 7.28223668 10.63512222]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.10206994e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 0.          0.        ]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.86345116 11.08433814]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.11093207 11.37716234]\n",
      "  [ 6.65109741  8.12412034]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.69426698 0.30573302]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 2]\n",
      "QTable: [[[ 5.79300864  4.39558621]\n",
      "  [ 5.86676366 10.06014217]]\n",
      "\n",
      " [[ 5.49635862  8.77406644]\n",
      "  [ 6.58137902  8.09556127]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 6.05324713  8.36052002]\n",
      "  [ 6.9864988  10.75129637]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.06858655e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [2 1]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.86345116 11.08433814]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.24031093e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.85006556 11.48909972]\n",
      "  [ 6.65109741  8.12412034]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.8478574e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 6.05324713  8.36052002]\n",
      "  [ 7.28784892 10.67616673]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.05696105e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.28784892 10.67616673]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.06858655e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.34686515 10.81263576]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.01139221e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.55906403 10.60855006]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.24515433e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.80315763 10.54769505]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.08290566e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.44032487 10.59890183]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.40402239e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.0134588  11.00195777]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.05929842e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.90116273 11.48411059]\n",
      "  [ 6.65109741  8.12412034]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0325525e-16 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.12554596 10.70023402]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.17267526e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.35694778 10.78107249]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.94434074e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.10555209 10.66254601]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.41101862e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.03893898 10.98418048]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.09475421e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.7928022  11.5070737 ]\n",
      "  [ 6.65109741  8.12412034]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.93046721e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.39499688 10.59629141]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.13130988e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.65549719 10.53666227]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.31206828e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.88994747 10.48299604]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.09495017e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 8.10095273 10.43469644]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.12423303e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 8.29085745 10.39122679]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.75293802e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.56100432 10.58928559]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.25710671e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.83659037 11.09491972]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 8.03912797e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.92080283 11.48925108]\n",
      "  [ 6.65109741  8.12412034]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 7.11513831 10.73261068]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.78661632 11.14316029]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.92080283 11.48925108]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.70665463 0.29334537]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[ 5.79300864  4.39558621]\n",
      "  [ 5.86676366 10.06014217]]\n",
      "\n",
      " [[ 5.59423464  9.18007749]\n",
      "  [ 6.58137902  8.09556127]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.87423797e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.37200403  8.61399527]\n",
      "  [ 6.87221568 10.82356946]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.01527167e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.76162929 11.16728058]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.10838226e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.70928769 11.57405333]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [9.5371151e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 4.99065373  8.77390987]\n",
      "  [ 6.87221568 10.82356946]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.66452972 11.21326162]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.60353012 11.61645445]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.76855755e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 4.99065373  8.77390987]\n",
      "  [ 6.70191973 10.89708938]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.01303275 10.96546656]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 4.99065373  8.77390987]\n",
      "  [ 6.75617296 10.83473131]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.17237963e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.74792842 11.12931506]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0000000e+00 3.8882301e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.51683037  9.35937522]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.50423424 11.64211975]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.37607348e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 6.75617296 10.83473131]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 6.67725785 10.86909168]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.04486735 10.9122942 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.00953207 10.78218251]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.86383993e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.30857886 10.70396426]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.07134533e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.57772097 10.63356784]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.25810019e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.20680743 10.6817016 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.22752759  8.44106029]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.40515665 10.72882774]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.48612668 10.61353144]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.305126   10.59829682]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.40515665 10.72882774]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.44447024e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.62462872 10.6037326 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.06894943e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.30241449 10.54064145]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.73328016 10.51812288]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.06706794e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.57217304 10.4865773 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.81495574 10.43791957]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.42271487e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.60078993 10.42620894]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.90695605 10.42954501]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0548215e-16 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.84071094 10.38358804]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.68313789 10.36361203]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.79266352e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.17582645 8.51206124]]\n",
      "\n",
      " [[4.8045813  8.72012977]\n",
      "  [6.16021017 9.95852259]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.46514733  8.9234377 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.50423424 11.64211975]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.81932974e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.46514733  8.9234377 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.82537228 11.46650469]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.33124787e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.9148241  10.32725082]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.09840249e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 8.12334169 10.29452574]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.63670061e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 8.31100752 10.26507317]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.76691994e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 8.47990677 10.23856585]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.88411734e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 8.63191609 10.21470927]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.000000e+00 1.000000e+00]\n",
      " [5.989595e-17 1.000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.78004212 10.09076367]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.69212193 10.01556677]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.39848868e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.56686983 10.05160854]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.64359241e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.17620988 10.59438526]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.46514733  8.9234377 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.93943407 11.25389576]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.25391887 10.26355837]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.28560651e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.96387857 10.79875093]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.93943407 11.25389576]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 8.24262057e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.87128458 11.23237123]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.83201263  8.77464084]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.52852698 10.23720254]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.000000e+00 1.000000e+00]\n",
      " [5.223965e-17 1.000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 4.80155774  8.70682079]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.52852698 10.23720254]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.13827206e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 5.83045312 10.89232396]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.94705308 11.21106663]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.41218966  8.44829141]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.52852698 10.23720254]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.000000e+00 1.000000e+00]\n",
      " [1.044793e-16 1.000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.41218966  8.44829141]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.77567428 10.21348228]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.07909158e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.41218966  8.44829141]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.99810685 10.19213405]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.54980149e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.99810685 10.19213405]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.09137912e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.6144693  10.5230156 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.54980149e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 8.19829617 10.17292065]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.68871073e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.7449358  10.29566656]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.17942014e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.90520897 10.37977443]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 7.97044222 10.26609991]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 8.173398   10.23948992]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.67143416e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.19779605  8.72084746]\n",
      "  [ 8.3560582  10.21554093]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.79818013e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 8.3560582  10.21554093]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.33142572 10.67431386]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.31594982 11.19174968]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.15963603e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 8.52045238 10.19398683]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.91225151e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.79279648 10.36671896]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.39330913e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.78283145 10.48384452]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.60953082 10.37689729]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.70653478e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.93522805 10.39268117]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.05603454e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.84857774 10.33920756]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.44604482e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 8.06371996 10.3052868 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.59532973e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.8059512  10.30971587]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 7.08999451 10.32022694]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 8.02535608 10.27874428]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.56870944e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.03827608  8.58107529]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 8.22282047 10.25086986]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.35513584  8.41862842]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 8.22282047 10.25086986]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.91967197e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.57137718 10.63681334]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.4471031  11.19622278]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.35513584  8.41862842]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.73996111 10.38016026]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.55980891e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.88167304 10.47047078]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.07413538e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.73996111 10.38016026]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.77511991e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 7.03682096 10.38729949]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.68470894 10.34499993]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.88277541e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.59725172 10.67213886]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.29743513 11.23611136]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [1.0664676e-16 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.48125516 10.44135551]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.15552595e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.31759991 10.83051398]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.14121963 11.29459789]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.26766754 10.55288335]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.56586697 10.63641033]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.26018056 10.5310058 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.23942134 10.83604432]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.60834729  9.46327715]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.97468204 11.35970126]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00755245e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.36098161  8.5123335 ]\n",
      "  [ 7.5341625  10.47790522]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.22787543e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.69415581  8.44732884]\n",
      "  [ 7.5341625  10.47790522]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.22787543e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.69415581  8.44732884]\n",
      "  [ 7.78074625 10.4301147 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.07979546e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.69415581  8.44732884]\n",
      "  [ 8.00267163 10.38710323]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.55296894e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.69415581  8.44732884]\n",
      "  [ 8.20240447 10.34839291]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 8.20240447 10.34839291]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.32946827e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.00958348 10.96190266]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.69404646  9.77551908]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.97468204 11.35970126]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.14576848e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.69404646  9.77551908]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.26830281 11.3192078 ]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 8.38216402 10.31355362]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.16325894e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 8.54394762 10.28219825]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.9285546e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 7.6758072  10.52586019]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.57511462 10.69129541]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.43023714  8.39750252]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 7.90822648 10.47327417]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.48743445e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 7.90822648 10.47327417]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 8.11740383 10.42594676]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.6325804e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 7.68874777 10.48949154]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.17582645  8.51206124]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.29487843 10.87333082]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.69404646  9.77551908]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.20965051 11.35540051]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.3351405e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 7.36232526 10.5940781 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.36794936e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.29487843 10.87333082]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.59236932 10.67429665]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.10863938e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 7.62609273 10.53467029]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.29166484e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.34284644  8.55808215]\n",
      "  [ 7.86348346 10.48120326]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.8168166   8.47152238]\n",
      "  [ 7.86348346 10.48120326]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.45638774e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.8168166   8.47152238]\n",
      "  [ 8.07713511 10.43308293]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.60463836e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.8168166   8.47152238]\n",
      "  [ 7.67551528 10.48626013]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.86639077 10.53165238]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.97861441  8.43026524]\n",
      "  [ 7.67551528 10.48626013]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.97861441  8.43026524]\n",
      "  [ 7.90796376 10.43763412]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.48725215e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.97861441  8.43026524]\n",
      "  [ 7.70053819 10.43147801]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.41705559 10.80798639]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.30881061e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.69404646  9.77551908]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.18504053 11.35029146]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.34332175e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.97861441  8.43026524]\n",
      "  [ 7.41709133 10.53893356]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.45272679e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.8045813   8.72012977]\n",
      "  [ 6.18254401 10.94410978]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.000000e+00 1.000000e+00]\n",
      " [1.000000e+00 4.291734e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.69404646  9.77551908]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.0593568  11.39071916]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.14664098e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.41709133 10.53893356]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.29000169e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.18254401 10.94410978]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.55810854 10.68762832]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.14664098e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.6753822  10.48504021]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.90784398 10.43653619]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 8.11705958 10.39288257]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 8.30535362 10.35359431]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.76299676e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.77592022 10.45184848]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.55060193e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.88942236 10.52455355]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.39562854e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.9983282  10.40666363]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 8.19849538 10.36599727]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.68884896e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 8.37864584 10.32939754]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.94509186 10.37452019]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.78049708e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.3684539  10.81856444]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.0593568  11.39071916]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.20771514 11.34516765]\n",
      "  [ 6.65140644  8.15636919]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.51301495e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.51992749 10.51468587]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.41900259e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.16731301 10.94734928]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.30746768e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.20771514 11.34516765]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0.33333333 0.66666667]\n",
      " [0.78237758 0.21762242]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 1]\n",
      "QTable: [[[ 5.79300864  4.39558621]\n",
      "  [ 5.86676366 10.06014217]]\n",
      "\n",
      " [[ 5.80508831  9.29591309]\n",
      "  [ 6.58137902  8.09556127]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.2179979e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.22688895 10.63628265]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.55886612e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.47441208 10.7281877 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.50420005 10.57265438]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.2070848e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.75378005 10.51538894]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.97840204 10.46385005]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.53612853e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.87840916  8.38806093]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 8.18056184 10.41746504]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.67640507e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.52896941  8.48530554]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 8.18056184 10.41746504]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.49252585e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.1058646   8.60536295]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 7.00943078 10.53107987]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.52896941  8.48530554]]\n",
      "\n",
      " [[ 5.46232481  8.59277653]\n",
      "  [ 7.89405323 10.42116447]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 7.00943078 10.53107987]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.86376965e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 7.16233668 10.43400572]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.52896941  8.48530554]]\n",
      "\n",
      " [[ 5.8887837   8.46485405]\n",
      "  [ 7.89405323 10.42116447]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.09551996e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.52896941  8.48530554]]\n",
      "\n",
      " [[ 6.10201314  8.40089281]\n",
      "  [ 7.89405323 10.42116447]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.09551996e-16 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 6.10201314  8.40089281]\n",
      "  [ 7.89405323 10.42116447]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.93973887e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 7.23878963 10.38546865]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.47759978e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 6.10201314  8.40089281]\n",
      "  [ 8.10464791 10.37904802]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 6.10201314  8.40089281]\n",
      "  [ 8.29418312 10.34114322]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 6.10201314  8.40089281]\n",
      "  [ 8.4647648  10.3070289 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.87361049e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 6.10201314  8.40089281]\n",
      "  [ 8.12789826 10.30770191]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.02291933e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 7.37055412 10.31581509]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.63986237e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 8.12789826 10.30770191]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.02286986e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.66836312 10.69597461]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.35501687 11.29566459]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.12797247e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.73129437 10.4322408 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.62710642e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.37618831 10.86625314]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.27002619 11.32072861]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.36466314e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.95816494 10.38901672]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.52955779 10.54100961]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.84873884e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.19610463 10.96141801]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.35070465e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.14683621 11.37676815]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.22468027e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.77660201 10.48690865]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.39610163e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.99894181 10.43821779]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.55038086e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.47791276 10.6036761 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.58921742 10.72217945]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.37464335 10.59070983]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.57218806e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.25334319 10.91179698]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.02327544 11.42466801]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.02343736e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.63717901 10.53163885]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.87346111 10.47847496]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.46331113e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 8.086115   10.43062747]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 8.2775035  10.38756472]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.74367186e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 8.44975315 10.34880825]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.86319406e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.72621385 10.53912292]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.71715714 10.67154766]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.07222756e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.95359247 10.48521063]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.66365909 10.51122438]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.66096407e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.9240422  10.54026358]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.62941938  8.41625506]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.60144643 10.47171762]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.80451942e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 7.00259967 10.45881884]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.27456303e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.85528826  8.39681458]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.60144643 10.47171762]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.05491261e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.85528826  8.39681458]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.60176308 10.41938635]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.85902962e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.41061001 10.79927662]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.05234295 11.38008854]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.27478275e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.85528826  8.39681458]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.84158677 10.37744771]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.85528826  8.39681458]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 8.0574281  10.33970294]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.85528826  8.39681458]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.59295805 10.48956212]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.44825427e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.74248823 10.59546316]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.26867303e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.85528826  8.39681458]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.49347432 10.48296632]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.67854105e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.87530825  8.7377088 ]\n",
      "  [ 6.2921813  10.849767  ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.0235612  11.3832308 ]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.19964233e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.49347432 10.48296632]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.19964233e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.74412689 10.43466969]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.38893596 10.55724164]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.36607784e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.95452665  8.7087612 ]\n",
      "  [ 6.2921813  10.849767  ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.95452665  8.7087612 ]\n",
      "  [ 6.05551513 10.97817582]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77076595  9.9440472 ]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.96735499 11.41451206]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.65004237 10.50151748]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.30828323e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.24722723 10.64202907]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.2018577e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89946937  8.74565093]\n",
      "  [ 6.05551513 10.97817582]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [8.4037154e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89946937  8.74565093]\n",
      "  [ 6.42664846 10.74589954]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.194273   10.61937435]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.45938318e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 5.02039407  8.67118528]\n",
      "  [ 6.42664846 10.74589954]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.91876636e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 5.02039407  8.67118528]\n",
      "  [ 6.59103343 10.62069951]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.4748457  10.55743691]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.37383622 10.52699826]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.57344817e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 5.02039407  8.67118528]\n",
      "  [ 6.18245871 10.87615458]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.14068431e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.77232497 10.03782842]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.96735499 11.41451206]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77232497 10.03782842]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.93321198 11.41805533]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.11662672e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.15990159 10.61396096]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.57988501e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 5.02039407  8.67118528]\n",
      "  [ 5.96451415 11.00529942]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.11699285e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77232497 10.03782842]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.83056663 11.45461205]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.96817975e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 6.96575646 10.70910025]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89600269  8.73771241]\n",
      "  [ 5.96451415 11.00529942]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.13871309e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89600269  8.73771241]\n",
      "  [ 5.81448373 11.08449453]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.04576832e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.77232497 10.03782842]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.7015859  11.51094612]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.26918081 10.63819022]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.04400744e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.54226273 10.5743712 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.04669922e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.09692486 10.72098341]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.03460857e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89600269  8.73771241]\n",
      "  [ 5.71617185 10.55737864]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.69509247  9.53404558]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.7015859  11.51094612]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.69509247  9.53404558]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.68956289 11.54386643]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.92448086e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.38723237 10.64888507]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.02518443e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 6.98008492 10.54739399]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.000000e+00 1.000000e+00]\n",
      " [7.932782e-17 1.000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89600269  8.73771241]\n",
      "  [ 5.63612291 10.09230755]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.62558323  9.08064102]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.68956289 11.54386643]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.94792733e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.62558323  9.08064102]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.63681542 11.49089081]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.84340687e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.28207643 10.49265459]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.01059112e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.55386879 10.44338913]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.24154941e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.03138356 10.25861759]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89600269  8.73771241]\n",
      "  [ 6.13061488 10.14960081]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [9.7580049e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 6.96793773 10.18914912]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.25396862e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89600269  8.73771241]\n",
      "  [ 5.82003361 10.67115673]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.62558323  9.08064102]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.6055828  11.32110505]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.83497807e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 6.81198231 10.36303725]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.03845957e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.35670461  8.51511342]]\n",
      "\n",
      " [[ 4.89600269  8.73771241]\n",
      "  [ 6.13480973 10.48079326]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 6.85991505 10.37383593]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.25687938e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.89600269 8.73771241]\n",
      "  [5.81763815 9.87265304]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.5630249   8.67257692]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.6055828  11.32110505]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.5630249   8.67257692]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.54675742 11.3100869 ]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.76002227e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 6.75701278 10.13597918]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.89600269 8.73771241]\n",
      "  [5.63402904 9.40535729]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.50672241  8.30531923]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.54675742 11.3100869 ]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.69767225e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.50672241  8.30531923]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.47618382 11.20943512]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.68861948e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.99922822  8.32677244]\n",
      "  [ 7.08131151 10.12238126]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.91364692e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.79426737 9.82333355]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.81878595e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.89600269 8.73771241]\n",
      "  [5.51970348 9.02480634]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.82105626e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.45605017  7.97478731]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.47618382 11.20943512]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 7.59973171e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.45605017  7.97478731]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.45579886 11.03405098]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.42894008e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.60501508 9.52158931]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.89600269 8.73771241]\n",
      "  [5.44227181 8.70231809]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 3.78589533e-17]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.41044515  7.67730858]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.45579886 11.03405098]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.41044515  7.67730858]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.36990546 10.82566121]\n",
      "  [ 6.64932698  8.21371066]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.58314989e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.47941626 9.24172189]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.89600269 8.73771241]\n",
      "  [5.81290241 9.0478478 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.4959982e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.5648691  9.24000007]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.89600269 8.73771241]\n",
      "  [6.03239884 9.21992393]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.55529301e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.90838219 9.31600006]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.58730621e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.86715063 9.3459696 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.37163511e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.89600269 8.73771241]\n",
      "  [6.26305967 9.3483498 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.93879918 9.41232472]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.34587066e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.26305967 9.3483498 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.34587066e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [5.7957079  8.74509833]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 2\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 5.41044515  7.67730858]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.36990546 10.82566121]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 2\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 0]\n",
      "QTable: [[[ 5.79300864  4.39558621]\n",
      "  [ 5.86676366 10.06014217]]\n",
      "\n",
      " [[ 5.80508831  9.29591309]\n",
      "  [ 6.56620918  7.81271052]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.81475914e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.78768275 9.20420169]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.11292705 9.05422984]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [7.10891448 9.28378152]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.93280033e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.99962806 9.2635827 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [5.72064159 8.59803835]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[1. 0.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 0\n",
      "Next state: 1\n",
      "Reward: [1 1]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.36990546 10.82566121]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.72612043e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.48480395 10.61826369]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [6.78807066 9.07100669]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [7.93898501e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.07554906 8.92742185]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.71017021e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.99922822 8.32677244]\n",
      "  [7.1092636  9.16390602]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.93304258e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.1092636  9.16390602]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.93304258e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [6.98485142 9.15292175]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.21575903e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.35670461 8.51511342]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.3317151  9.12487963]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.8467143e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.02511175 9.22641273]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.21103834 7.90750856]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.3317151  9.12487963]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.39350993e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.21103834 7.90750856]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.47590225 9.2530049 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.87465051e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.32260058 9.30377145]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.01621497e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.25166119 9.35308769]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.49355986e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.21103834 7.90750856]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.6386156  9.36773753]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.03185076e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.28127683 9.42363886]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[3.44437283 5.05086494]\n",
      "  [5.21103834 7.90750856]]\n",
      "\n",
      " [[4.95322522 8.10819613]\n",
      "  [6.01322938 9.93117424]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.65491271 10.57858739]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.05240074e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.04593017 9.68428912]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.34503214e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 5.76857977 10.19702207]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 1.17716518e-16]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.64582842 10.66300934]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.88909619e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [6.83039699 9.92095339]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 5.64262126 10.36371477]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.91758044e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.55507301 10.79988603]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.73954001e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.1473573  9.92885805]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.91895079e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.43262157 9.93597225]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.15741725e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[6.53315885 8.46422809]\n",
      "  [5.92503386 8.39159382]]\n",
      "\n",
      " [[5.84331955 7.82894863]\n",
      "  [7.68935941 9.94237502]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.33556491e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.84331955  7.82894863]\n",
      "  [ 7.10172821 10.11667342]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [3.91535502e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 6.16200191 10.22852675]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.92781386e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.84331955  7.82894863]\n",
      "  [ 7.39155539 10.10500608]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.84331955  7.82894863]\n",
      "  [ 7.16057846 10.14391374]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.27574775e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 6.44523234 10.17182887]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.84331955  7.82894863]\n",
      "  [ 7.44452061 10.12952237]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.16567387e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.84331955  7.82894863]\n",
      "  [ 7.70006855 10.11657013]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.06859917e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.84331955  7.82894863]\n",
      "  [ 7.42812721 10.12701661]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.47227834e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 5.94464537 10.40586885]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.85460622e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.74878739 10.95074966]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.84331955  7.82894863]\n",
      "  [ 7.09192175 10.22585585]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.12492635e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 6.30909139 10.29327676]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.09192175 10.22585585]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.37781158e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 6.4913144  10.23698072]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.92100926e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.38272958 10.20327026]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.12279773e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.64445662 10.18294324]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.30440734e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.88001096 10.16464891]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.92503386  8.39159382]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.53653124 10.17711674]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 4.95322522  8.10819613]\n",
      "  [ 6.76026969 10.18933706]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.22951907e-17 1.00000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.53653124 10.17711674]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.04590381e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.4723735  10.16429319]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.69087942e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.76026969 10.18933706]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.38175883e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.1796498  10.47496839]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.86334309 11.04109211]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.72513615 10.14786388]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.000000e+00 1.000000e+00]\n",
      " [1.072078e-16 1.000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.95262253 10.13307749]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.10364808e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.44817119 10.2565261 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.28799343e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.56909338 10.34009464]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.35172294 10.26430091]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.5582242e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.12988393 10.58648416]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.87236072 11.12626642]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.10128255e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.12781504 10.36674412]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.27603843e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 5.91388625 10.74374865]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.78730638 11.20983085]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.94591523e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.41503354 10.33006971]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.67353018 10.29706273]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.06491624e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.20231959 10.44603083]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.33787096 10.55028665]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.99761315e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.13630818 10.44313007]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.79556284e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 5.21103834  7.90750856]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 5.98385803 10.75907567]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.74817646 11.28216746]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 6.9616973  10.5251953 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.1521356e-17 1.0000000e+00]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 5.98385803 10.75907567]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.27660794 10.58961596]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.44919437e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.26552757 10.47267577]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.14340696 10.47218427]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.35527166e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 5.93757455 10.80767496]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.73145102 11.32995744]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.9567343e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.97712942  8.26664361]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 6.9467333  10.55916212]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 5.76136768 10.93582046]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.97699305e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.64441883 11.38864357]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 0\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 6.9467333  10.55916212]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.19932557e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.15937716 10.69157508]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.82026454e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.25205997 10.50324591]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.52685398 10.45292132]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.22280412e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.22717785 10.50309069]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.27392646e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 5.83745611 10.90124496]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.71308056 11.39555806]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.01486203e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.50446007 10.45278162]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.20726522e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.75401406 10.40750346]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.07608562e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.97861265 10.36675311]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 8.18075139 10.3300778 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.67653659e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.44529633  8.03178502]\n",
      "  [ 7.42535814 10.52553689]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.05054886e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 6.38887131 10.66083724]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.27819669  8.2802274 ]\n",
      "  [ 7.42535814 10.52553689]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [8.86634004e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.18072049  8.12983289]\n",
      "  [ 5.97966788 10.88864184]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.82668353 11.40799378]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.15237723e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.27819669  8.2802274 ]\n",
      "  [ 7.10454622 10.61822518]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 5.97966788 10.88864184]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [4.1492281e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 5.82050735 11.00751843]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 4.04307389e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.75516026 11.45128696]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.92976925e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.27819669  8.2802274 ]\n",
      "  [ 7.3940916  10.55640266]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.3940916  10.55640266]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 5.71231778 11.084274  ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.99344464e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.83521677 11.44820455]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 6.98197291 10.71191093]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [3.9637167e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 5.6902456  11.12141882]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.71039755 11.50886665]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.84471693e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.28377562 10.64071984]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.01082692e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.55539806 10.57664785]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.24261055e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.05379727 10.73689146]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 6.16664171 10.85546599]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.89455509e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 6.99355532 10.71063212]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 5.86747987 11.03127965]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.6526209  11.53868617]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.85275383e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.29419978 10.63956891]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.06136784e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.56477981 10.57561202]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.12938185 10.70031787]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.07138203e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 6.28549268 10.79576698]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.89400485e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.078888   10.66846573]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 6.47430154 10.66526978]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.91196528e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.3709992  10.60161915]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.11465814e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.63389928 10.54145724]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.29708172e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.87050935 10.48731151]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.46126293e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 8.08345841 10.43858036]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.60902603e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.63144982 10.48539809]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 6.7897307  10.52679413]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.86830484 10.43685828]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.45973325e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 8.08147436 10.39317246]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 8.27332692 10.35385521]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 8.44599423 10.31846969]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.17211716e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.93888939 10.3699525 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.42264419e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 6.15591371 10.87887153]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 3.92229367e-17]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.00186621 11.41732408]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.50871112e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 8.14500045 10.33295725]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.6517294e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.53486571 10.51802724]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.27152321e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 5.97870334 11.0063654 ]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.01487939 11.41587294]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.22836337e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.15891419 10.66155978]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 4.98222739  8.42037318]\n",
      "  [ 6.35291734 10.76780661]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.44302277 10.5954038 ]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.16463453e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.69872049 10.53586342]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.39052718 10.57505435]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.40822194e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.03228063  8.51730923]\n",
      "  [ 6.35291734 10.76780661]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.40822194e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.03228063  8.51730923]\n",
      "  [ 6.08241043 10.95025248]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 5.96365057 11.43795821]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.1282084e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.12822776 10.66762817]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.03228063  8.51730923]\n",
      "  [ 6.39249632 10.74217751]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.94620162e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.41540499 10.60086535]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.02909417e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.26470102 10.59730368]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.43568537e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 6.39249632 10.74217751]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.43568537e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 6.60212857 10.61001023]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.04089896e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.27320194 10.54265593]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.58114697e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 6.71034506 10.52206748]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.04679766e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.32073899 10.48015496]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 6.78346813 10.45309573]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.58866509 10.43213946]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.0000000e+00 1.0000000e+00]\n",
      " [5.2656942e-17 1.0000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.5077198  10.39730802]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [4.70697656e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 6.89482198 10.38547107]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.20952711e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 4.96730129  8.54312108]\n",
      "  [ 7.75694782 10.35757722]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [1.07649276e-16 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 0\n",
      "Reward: [0 0]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.24157944  8.42574897]\n",
      "  [ 7.75694782 10.35757722]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.56848764e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 7.05019012 10.33576642]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.24157944  8.42574897]\n",
      "  [ 7.69854996 10.31309518]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 7.10451504 10.29312128]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.34194214e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.24157944  8.42574897]\n",
      "  [ 7.69108099 10.2737961 ]]]\n",
      "State: 0\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [9.85949522e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 1\n",
      "Reward: [1 2]\n",
      "QTable: [[[ 3.44437283  5.05086494]\n",
      "  [ 4.99906239  8.25738455]]\n",
      "\n",
      " [[ 5.07313885  8.55552562]\n",
      "  [ 6.43771775 10.72174393]]]\n",
      "State: 1\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [1. 0.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 0\n",
      "Next state: 3\n",
      "Reward: [0 3]\n",
      "QTable: [[[ 5.36940064  7.40957772]\n",
      "  [ 7.55771924  6.77934003]]\n",
      "\n",
      " [[ 6.05825768 11.32849755]\n",
      "  [ 6.64669881  8.82522057]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0. 1.]\n",
      " [0. 1.]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 3\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.24157944  8.42574897]\n",
      "  [ 7.9219729  10.24641649]]]\n",
      "State: 3\n",
      "Nash equilibrium: [[0.00000000e+00 1.00000000e+00]\n",
      " [5.49697294e-17 1.00000000e+00]]\n",
      "Player 1 action: 1\n",
      "Player 2 action: 1\n",
      "Next state: 0\n",
      "Reward: [2 2]\n",
      "QTable: [[[ 6.53315885  8.46422809]\n",
      "  [ 5.29311179  8.50764999]]\n",
      "\n",
      " [[ 5.24157944  8.42574897]\n",
      "  [ 7.53607355 10.41190582]]]\n",
      "Expected reward: [1.053  1.2976]\n",
      "QTable: [[[[ 3.44437283  5.05086494]\n",
      "   [ 4.99906239  8.25738455]]\n",
      "\n",
      "  [[ 5.07313885  8.55552562]\n",
      "   [ 6.43771775 10.72174393]]]\n",
      "\n",
      "\n",
      " [[[ 5.36940064  7.40957772]\n",
      "   [ 7.55771924  6.77934003]]\n",
      "\n",
      "  [[ 6.05825768 11.32849755]\n",
      "   [ 6.64669881  8.82522057]]]\n",
      "\n",
      "\n",
      " [[[ 5.79300864  4.39558621]\n",
      "   [ 5.86676366 10.06014217]]\n",
      "\n",
      "  [[ 5.80508831  9.29591309]\n",
      "   [ 6.56620918  7.81271052]]]\n",
      "\n",
      "\n",
      " [[[ 6.53315885  8.46422809]\n",
      "   [ 5.29311179  8.50764999]]\n",
      "\n",
      "  [[ 5.24157944  8.42574897]\n",
      "   [ 7.53607355 10.41190582]]]]\n"
     ]
    }
   ],
   "source": [
    "#Simulate plays with nash policy\n",
    "ALPHA = 0.5\n",
    "GAMMA = 0.8\n",
    "EPSILON = 0.1\n",
    "# n_games = 5000\n",
    "\n",
    "qTable = np.zeros((Q, A, A, N))\n",
    "\n",
    "state = 0\n",
    "totalReward = np.array([0, 0])\n",
    "diffs = []\n",
    "NashQRewards = []\n",
    "\n",
    "for _ in range(n_games):\n",
    "    print(\"State:\", state)\n",
    "    nashEq = np.abs(computeNashEq(state, qTable))\n",
    "    print(\"Nash equilibrium:\", nashEq)\n",
    "    player1_action = np.random.choice(A, p=nashEq[0]) if np.random.rand() > EPSILON else np.random.choice(A)\n",
    "    player2_action = np.random.choice(A, p=nashEq[1]) if np.random.rand() > EPSILON else np.random.choice(A)\n",
    "    print(\"Player 1 action:\", player1_action)\n",
    "    print(\"Player 2 action:\", player2_action)\n",
    "    next_state = np.random.choice(range(Q), p=TRANSITION_MATRIX[player1_action, player2_action, state])\n",
    "    print(\"Next state:\", next_state)\n",
    "    r = reward(state, player1_action, player2_action)\n",
    "    print(\"Reward:\", r)\n",
    "\n",
    "    next_NashEq = computeNashEq(next_state, qTable)\n",
    "    next_qVal_0 = expectedPayoff(qTable[next_state, :, :, 0], next_NashEq[0], next_NashEq[1])\n",
    "    next_qVal_1 = expectedPayoff(qTable[next_state, :, :, 1], next_NashEq[0], next_NashEq[1])\n",
    "\n",
    "    oldQ = qTable[state, player1_action, player2_action].copy()\n",
    "\n",
    "    qTable[state, player1_action, player2_action, 0] = (1 - ALPHA) * qTable[state, player1_action, player2_action, 0] + ALPHA * (r[0] + GAMMA * next_qVal_0)\n",
    "    qTable[state, player1_action, player2_action, 1] = (1 - ALPHA) * qTable[state, player1_action, player2_action, 1] + ALPHA * (r[1] + GAMMA * next_qVal_1)\n",
    "    \n",
    "    diffs.append(qTable[state, player1_action, player2_action] - oldQ)\n",
    "\n",
    "    print(\"QTable:\", qTable[state])\n",
    "    state = next_state\n",
    "    totalReward += r\n",
    "    NashQRewards.append(r)\n",
    "print(\"Expected reward:\", totalReward/n_games)\n",
    "print(\"QTable:\", qTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "diffs = np.array(diffs)\n",
    "print(diffs.shape)\n",
    "diffs\n",
    "\n",
    "randomRewards = np.array(randomRewards)\n",
    "localNashRewards = np.array(localNashRewards)\n",
    "NashQRewards = np.array(NashQRewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"QTable diffs over time player 1\")\n",
    "sns.lineplot(data=diffs[:, 0])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"QTable diffs over time player 2\")\n",
    "sns.lineplot(data=diffs[:, 1])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Rewards over time per player 0\")\n",
    "sns.lineplot(data=randomRewards[:, 0], label=\"Random\")\n",
    "sns.lineplot(data=localNashRewards[:, 0], label=\"Local Nash\")\n",
    "sns.lineplot(data=NashQRewards[:, 0], label=\"Nash Q\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# smooth the data\n",
    "window = 500\n",
    "cumsum = np.cumsum(randomRewards[:, 0])\n",
    "cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "randomRewardsSmooth = cumsum[window - 1:] / window\n",
    "\n",
    "cumsum = np.cumsum(localNashRewards[:, 0])\n",
    "cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "localNashRewardsSmooth = cumsum[window - 1:] / window\n",
    "\n",
    "cumsum = np.cumsum(NashQRewards[:, 0])\n",
    "cumsum[window:] = cumsum[window:] - cumsum[:-window]\n",
    "NashQRewardsSmooth = cumsum[window - 1:] / window\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f\"Smoothed Rewards over time per player 0 - windowSize {window}\")\n",
    "sns.lineplot(data=randomRewardsSmooth, label=\"Random\")\n",
    "sns.lineplot(data=localNashRewardsSmooth, label=\"Local Nash\")\n",
    "sns.lineplot(data=NashQRewardsSmooth, label=\"Nash Q\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
